<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-10-28T15:47:08+08:00</updated><id>http://localhost:4000/</id><title type="html">被水淹死的鱼</title><subtitle>个人技术微博</subtitle><author><name>true</name></author><entry><title type="html">Non-parameter model: Decision Tree</title><link href="http://localhost:4000/2018/10/28/Decision-Tree.html" rel="alternate" type="text/html" title="Non-parameter model: Decision Tree" /><published>2018-10-28T00:00:00+08:00</published><updated>2018-10-28T00:00:00+08:00</updated><id>http://localhost:4000/2018/10/28/Decision%20Tree</id><content type="html" xml:base="http://localhost:4000/2018/10/28/Decision-Tree.html">&lt;p&gt;[TOC]&lt;/p&gt;

&lt;h1 id=&quot;decision-tree-algorithm&quot;&gt;Decision Tree algorithm&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;决策树（decision tree）算法基于特征属性进行分类，其主要的优点：模型具有可读性，计算量小，分类速度快。决策树算法包括了由Quinlan提出的ID3与C4.5，Breiman等提出的CART。其中，C4.5是基于ID3的，对分裂属性的目标函数做出了改进。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;一-基础知识&quot;&gt;一. 基础知识&lt;/h2&gt;

&lt;h3 id=&quot;11-决策树模型&quot;&gt;1.1 决策树模型&lt;/h3&gt;

&lt;p&gt;决策树是一种通过对特征属性的分类对样本进行分类的树形结构，包括有向边与三类节点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;根节点（root node）&lt;/code&gt;：表示第一个特征属性，只有出边没有入边；&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;内部节点（internal node）&lt;/code&gt;：表示特征属性，有一条入边至少两条出边；&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;叶子节点（leaf node）&lt;/code&gt;：表示类别，只有一条入边没有出边。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/tree/tree_1.png&quot; style=&quot;zoom:50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上图给出了（二叉）决策树的示例。决策树具有以下特点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;对于二叉决策树而言，可以看作是if-then规则集合，由决策树的根节点到叶子节点对应于一条分类规则;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;分类规则是&lt;strong&gt;互斥并且完备&lt;/strong&gt;的，所谓&lt;strong&gt;互斥&lt;/strong&gt;即每一条样本记录不会同时匹配上两条分类规则，所谓&lt;strong&gt;完备&lt;/strong&gt;即每条样本记录都在决策树中都能匹配上一条规则。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;分类的本质是对特征空间的划分，如下图所示，&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/tree/tree_2.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;12-决策树学习&quot;&gt;1.2 决策树学习&lt;/h3&gt;

&lt;p&gt;决策树学习的本质是从训练数据集中归纳出一组分类规则。但随着分裂属性次序的不同，所得到的决策树也会不同。如何得到一棵决策树既对训练数据有较好的拟合，又对未知数据有很好的预测呢？&lt;/p&gt;

&lt;p&gt;首先，我们要解决两个问题：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;如何选择较优的特征属性进行分裂？每一次特征属性的分裂，相当于对训练数据集进行再划分，对应于一次决策树的生长。ID3算法定义了目标函数来进行特征选择。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;什么时候应该停止分裂？有两种自然情况应该停止分裂，一是该节点对应的所有样本记录均属于同一类别，二是该节点对应的所有样本的特征属性值均相等。但除此之外，是不是还应该其他情况停止分裂呢？&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;13-information-theory信息论&quot;&gt;1.3 information Theory(信息论)&lt;/h3&gt;

&lt;h4 id=&quot;131-自信息&quot;&gt;1.3.1 自信息&lt;/h4&gt;

&lt;p&gt;       在信息论中，&lt;code class=&quot;highlighter-rouge&quot;&gt;自信息&lt;/code&gt;（英语：self-information），由克劳德·香农提出，是与概率空间中的单一事件或离散随机变量的值相关的信息量的量度。它用信息的单位表示，例如 bit、nat或是hart，使用哪个单位取决于在计算中使用的对数的底。自信息的期望值就是信息论中的&lt;code class=&quot;highlighter-rouge&quot;&gt;熵&lt;/code&gt;，它&lt;strong&gt;反映了随机变量采样时的平均不确定程度&lt;/strong&gt;。  &lt;br /&gt;
       由定义，当信息被拥有它的实体传递给接收它的实体时，仅当接收实体不知道信息的先验知识时信息才得到传递。如果接收实体事先知道了消息的内容，这条消息所传递的信息量就是&lt;code class=&quot;highlighter-rouge&quot;&gt;0&lt;/code&gt;。只有当接收实体对消息对先验知识少于&lt;code class=&quot;highlighter-rouge&quot;&gt;100%&lt;/code&gt;时，消息才真正传递信息。  &lt;br /&gt;
       因此，一个随机产生的事&lt;code class=&quot;highlighter-rouge&quot;&gt;ω_n&lt;/code&gt;所包含的自信息数量，只与事件发生的机率相关。事件发生的机率越低，在事件真的发生时，接收到的信息中，包含的自信息越大。&lt;code class=&quot;highlighter-rouge&quot;&gt;ω_n&lt;/code&gt;的自信息量：
\[ I(ω_n) = f(P(ω_n)) \] 
如果 P(ω_n) = 1，则 I(ω_n) = 0。如果 P(ω_n) &amp;lt; 1, 则 I(ω_n) &amp;gt; 0。此外，根据定义，自信息的量度是非负的而且是可加的。如果事件 &lt;code class=&quot;highlighter-rouge&quot;&gt;C&lt;/code&gt; 是两个独立事件 &lt;code class=&quot;highlighter-rouge&quot;&gt;A&lt;/code&gt; 和 &lt;code class=&quot;highlighter-rouge&quot;&gt;B&lt;/code&gt; 的交集，那么宣告 &lt;code class=&quot;highlighter-rouge&quot;&gt;C&lt;/code&gt; 发生的信息量就等于分别宣告事件 &lt;code class=&quot;highlighter-rouge&quot;&gt;A&lt;/code&gt; 和事件 &lt;code class=&quot;highlighter-rouge&quot;&gt;B&lt;/code&gt; 的信息量的和：
\[ I(C) = I(A∩B) = I(A) + I(B) \] 
因为 &lt;code class=&quot;highlighter-rouge&quot;&gt;A&lt;/code&gt; 和 &lt;code class=&quot;highlighter-rouge&quot;&gt;B&lt;/code&gt; 是独立事件，所以 &lt;code class=&quot;highlighter-rouge&quot;&gt;C&lt;/code&gt; 的概率为： 
\[ P(C) = P(A∩B) = P(A) · P(B) \] 
应用函数 &lt;code class=&quot;highlighter-rouge&quot;&gt;f(·)&lt;/code&gt; 会得到：
\[ I(C) = I(A) + I(B) \]
\[ f(P(C)) = f(P(A)) + f(P(B)) = f(P(A) · P(B)) \] 
所以函数 &lt;code class=&quot;highlighter-rouge&quot;&gt;f(·)&lt;/code&gt; 具有性质 
\[ f(x·y) = f(x) + f(y) \] 
而对数函数正好有这个性质，不同的底的对数函数之间的区别只差一个常数：
\[ f(x) = Klog(x) \] 
由于事件的概率总在 &lt;code class=&quot;highlighter-rouge&quot;&gt;0&lt;/code&gt; 和 &lt;code class=&quot;highlighter-rouge&quot;&gt;1&lt;/code&gt; 之间，而信息量不能为负，所以 &lt;code class=&quot;highlighter-rouge&quot;&gt;K&amp;lt;0&lt;/code&gt; 。考虑到这些性质，假设事件 &lt;code class=&quot;highlighter-rouge&quot;&gt;ω_n&lt;/code&gt; 发生的机率是  &lt;code class=&quot;highlighter-rouge&quot;&gt;P(ω_n)&lt;/code&gt; ,自信息量的 &lt;code class=&quot;highlighter-rouge&quot;&gt;I(ω_n)&lt;/code&gt; 的定义为：
\[ I(ω_n) = -log(P(ω_n)) = log(\frac{1}{P(ω_n)}) \]
事件 &lt;code class=&quot;highlighter-rouge&quot;&gt;ω_n&lt;/code&gt; 的概率越小, 它发生后的自信息量越大。此定义符合上述条件。在上面的定义中，没有指定的对数的基底：如果以 &lt;code class=&quot;highlighter-rouge&quot;&gt;2&lt;/code&gt; 为底，单位是 &lt;code class=&quot;highlighter-rouge&quot;&gt;bit&lt;/code&gt;。当使用以 &lt;code class=&quot;highlighter-rouge&quot;&gt;e&lt;/code&gt; 为底的对数时，单位将是 &lt;code class=&quot;highlighter-rouge&quot;&gt;nat&lt;/code&gt;。对于基底为 &lt;code class=&quot;highlighter-rouge&quot;&gt;10&lt;/code&gt; 的对数，单位是 &lt;code class=&quot;highlighter-rouge&quot;&gt;hart&lt;/code&gt;。信息量的大小不同于信息作用的大小，这不是同一概念。信息量只表明不确定性的减少程度，至于对接收者来说，所获得的信息可能事关重大，也可能无足轻重，这是信息作用的大小。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;和熵的联系
熵是离散随机变量的自信息的期望值。但有时候熵也会被称作是随机变量的自信息，可能是因为熵满足 H(X) = I(X;X)，而 I(X;X) 是 X 和它自己的互信息。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;132-互信息&quot;&gt;1.3.2 互信息&lt;/h4&gt;

&lt;p&gt;       在概率论和信息论中，两个随机变量的 &lt;code class=&quot;highlighter-rouge&quot;&gt;互信息（Mutual Information，简称MI）&lt;/code&gt; 或&lt;code class=&quot;highlighter-rouge&quot;&gt; 转移信息（transinformation）&lt;/code&gt; 是变量间相互依赖性的量度。不同于相关系数，互信息并不局限于实值随机变量，它更加一般且决定着联合分布 &lt;code class=&quot;highlighter-rouge&quot;&gt;p(X,Y)&lt;/code&gt; 和分解的边缘分布的乘积 &lt;code class=&quot;highlighter-rouge&quot;&gt;p(X)p(Y)&lt;/code&gt; 的相似程度。互信息是 &lt;code class=&quot;highlighter-rouge&quot;&gt;点间互信息（PMI）&lt;/code&gt; 的期望值。互信息最常用的单位是 &lt;code class=&quot;highlighter-rouge&quot;&gt;bit&lt;/code&gt;。  &lt;br /&gt;
       一般地，两个离散随机变量 &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt; 和 &lt;code class=&quot;highlighter-rouge&quot;&gt;Y&lt;/code&gt; 的互信息可以定义为：
\[ I(X;Y) = \sum_{y∈Y}\sum_{x∈X}{P(x,y)log(\frac{P(x,y)}{P(x)·P(y)})} \] 
其中 &lt;code class=&quot;highlighter-rouge&quot;&gt;p(x,y)&lt;/code&gt; 是 &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt; 和 &lt;code class=&quot;highlighter-rouge&quot;&gt;Y&lt;/code&gt; 的联合概率分布函数，而 &lt;code class=&quot;highlighter-rouge&quot;&gt;P(x)&lt;/code&gt; 和 &lt;code class=&quot;highlighter-rouge&quot;&gt;P(y)&lt;/code&gt; 分别是 &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt; 和 &lt;code class=&quot;highlighter-rouge&quot;&gt;Y&lt;/code&gt; 的边缘概率分布函数。  &lt;br /&gt;
在连续随机变量的情形下，求和被替换成了二重定积分：
\[ I(X;Y) = \int_{Y}\int_{X}{P(x,y)log(\frac{P(x,y)}{P(x)·P(y)})} {\rm d}x{\rm d}y \] 
其中 &lt;code class=&quot;highlighter-rouge&quot;&gt;P(x,y)&lt;/code&gt; 当前是 &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt; 和 &lt;code class=&quot;highlighter-rouge&quot;&gt;Y&lt;/code&gt; 的联合概率密度函数，而 &lt;code class=&quot;highlighter-rouge&quot;&gt;P(x)&lt;/code&gt; 和 &lt;code class=&quot;highlighter-rouge&quot;&gt;P(y)&lt;/code&gt; 分别是 &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt; 和 &lt;code class=&quot;highlighter-rouge&quot;&gt;Y&lt;/code&gt; 的边缘概率密度函数。如果对数以 &lt;code class=&quot;highlighter-rouge&quot;&gt;2&lt;/code&gt; 为基底，互信息的单位是 &lt;code class=&quot;highlighter-rouge&quot;&gt;bit&lt;/code&gt;。  &lt;br /&gt;
       直观上，互信息度量 &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt; 和 &lt;code class=&quot;highlighter-rouge&quot;&gt;Y&lt;/code&gt; 共享的信息：它度量知道这两个变量其中一个，对另一个不确定度减少的程度。例如，如果 &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt; 和 &lt;code class=&quot;highlighter-rouge&quot;&gt;Y&lt;/code&gt; 相互独立，则知道 &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt; 不对 &lt;code class=&quot;highlighter-rouge&quot;&gt;Y&lt;/code&gt; 提供任何信息，反之亦然，所以它们的互信息为零。在另一个极端，如果 &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt; 是 &lt;code class=&quot;highlighter-rouge&quot;&gt;Y&lt;/code&gt; 的一个确定性函数，且 &lt;code class=&quot;highlighter-rouge&quot;&gt;Y&lt;/code&gt; 也是 &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt; 的一个确定性函数，那么传递的所有信息被 &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt; 和 &lt;code class=&quot;highlighter-rouge&quot;&gt;Y&lt;/code&gt; 共享：知道 &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt; 决定 &lt;code class=&quot;highlighter-rouge&quot;&gt;Y&lt;/code&gt; 的值，反之亦然。因此，在此情形互信息与 &lt;code class=&quot;highlighter-rouge&quot;&gt;Y&lt;/code&gt;（或 &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt;）单独包含的不确定度相同，称作 &lt;code class=&quot;highlighter-rouge&quot;&gt;Y&lt;/code&gt;（或 &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt;）的熵。而且，这个互信息与 &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt; 的熵和 &lt;code class=&quot;highlighter-rouge&quot;&gt;Y&lt;/code&gt; 的熵相同。(这种情形的一个非常特殊的情况是当 &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt; 和 &lt;code class=&quot;highlighter-rouge&quot;&gt;Y&lt;/code&gt; 为相同随机变量时。)  &lt;br /&gt;
       互信息是 &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt; 和 &lt;code class=&quot;highlighter-rouge&quot;&gt;Y&lt;/code&gt; 的联合分布相对于假定 &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt; 和 &lt;code class=&quot;highlighter-rouge&quot;&gt;Y&lt;/code&gt; 独立情况下的联合分布之间的内在依赖性。 于是互信息以下面方式度量依赖性：&lt;code class=&quot;highlighter-rouge&quot;&gt;I(X; Y) = 0&lt;/code&gt; 当且仅当 &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt; 和 &lt;code class=&quot;highlighter-rouge&quot;&gt;Y&lt;/code&gt; 为独立随机变量。从一个方向很容易看出：当 &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt; 和 &lt;code class=&quot;highlighter-rouge&quot;&gt;Y&lt;/code&gt; 独立时，&lt;code class=&quot;highlighter-rouge&quot;&gt;p(x,y) = p(x) p(y)&lt;/code&gt;，因此，
\[ log(\frac{P(A,B)}{P(A)·P(B)}) = log(1) = 0 \]
此外，互信息是非负的（即 &lt;code class=&quot;highlighter-rouge&quot;&gt;I(X;Y) ≥ 0&lt;/code&gt;），而且是对称的（即 &lt;code class=&quot;highlighter-rouge&quot;&gt;I(X;Y) = I(Y;X)&lt;/code&gt;）。  &lt;br /&gt;
       另外，互信息可以简单的表示成：
\[ I(X;Y) = H(Y) - H(Y|X) = H(X,Y) - H(X|Y) - H(Y|X) \] 
其中 &lt;code class=&quot;highlighter-rouge&quot;&gt;H(X)&lt;/code&gt; 和 &lt;code class=&quot;highlighter-rouge&quot;&gt;H(Y)&lt;/code&gt; 是&lt;strong&gt;边缘熵&lt;/strong&gt;，&lt;code class=&quot;highlighter-rouge&quot;&gt;H(X|Y)&lt;/code&gt; 和 &lt;code class=&quot;highlighter-rouge&quot;&gt;H(Y|X)&lt;/code&gt; 是&lt;strong&gt;条件熵&lt;/strong&gt;，而 &lt;code class=&quot;highlighter-rouge&quot;&gt;H(X,Y)&lt;/code&gt; 是 &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt; 和 &lt;code class=&quot;highlighter-rouge&quot;&gt;Y&lt;/code&gt; 的&lt;strong&gt;联合熵&lt;/strong&gt;。直观地说，如果把熵 H(Y) 看作一个随机变量于不确定度的量度，那么 H(Y|X) 就是”在已知 X 事件后Y事件会发生”的不确定度。这证实了互信息的直观意义为: “因X而有Y事件”的熵(基于已知随机变量的不确定性) 在”Y事件”的熵之中具有多少影响地位(“Y事件所具有的不确定性” 其中包含了多少 “Y|X事件所具有的不确性” )，意即”Y具有的不确定性”有多少程度是起因于X事件。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;舉例來說，當 &lt;code class=&quot;highlighter-rouge&quot;&gt;I(X;Y) = 0&lt;/code&gt;時，也就是 &lt;code class=&quot;highlighter-rouge&quot;&gt;H(Y) = H(Y|X)&lt;/code&gt;時，即代表此時 &lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;Y的不確定性&quot;&lt;/code&gt; 即為 &lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;Y|X的不確定性&quot;&lt;/code&gt;，這說明了互信息的具體意義是在度量兩個事件彼此之間的關聯性。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;所以具体的解释就是：互信息越小，两个来自不同事件空间的随机变量彼此之间的关联性越低；互信息越高，关联性则越高。&lt;/p&gt;

&lt;h4 id=&quot;133-信息熵&quot;&gt;1.3.3 信息熵&lt;/h4&gt;

&lt;p&gt;       在信息论中，&lt;code class=&quot;highlighter-rouge&quot;&gt;熵&lt;/code&gt;（英语：entropy）是接收的每条消息中包含的信息的平均量，又被称为&lt;strong&gt;信息熵&lt;/strong&gt;、&lt;strong&gt;信源熵&lt;/strong&gt;、&lt;strong&gt;平均自信息量&lt;/strong&gt;。这里，“消息”代表来自分布或数据流中的事件、样本或特征。（熵最好理解为不确定性的量度而不是确定性的量度，因为越随机的信源的熵越大。）
\[ H(X) = E[I(X)] = E[-ln[P(X)]] \] 
其中，&lt;code class=&quot;highlighter-rouge&quot;&gt;P&lt;/code&gt; 为 &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt; 的概率质量函数（probability mass function），&lt;code class=&quot;highlighter-rouge&quot;&gt;E&lt;/code&gt; 为期望函数，而 &lt;code class=&quot;highlighter-rouge&quot;&gt;I(X)&lt;/code&gt; 是 &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt; 的信息量（又称为自信息）。&lt;code class=&quot;highlighter-rouge&quot;&gt;I(X)&lt;/code&gt; 本身是个随机变数。当取自有限的样本时，&lt;strong&gt;熵&lt;/strong&gt;的公式可以表示为：
\[ H(X) = \sum_{i} {P(x_i)I(x_i)} = -\sum_{i} {P(x_i)log_b{P(x_i)}} \] 
可以定义事件 &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt; 与 &lt;code class=&quot;highlighter-rouge&quot;&gt;Y&lt;/code&gt; 分别取 &lt;code class=&quot;highlighter-rouge&quot;&gt;x_i&lt;/code&gt; 和 &lt;code class=&quot;highlighter-rouge&quot;&gt;y_j&lt;/code&gt; 时的&lt;strong&gt;条件熵&lt;/strong&gt;为：
\[ {H(X|Y)} = -\sum_{i,j}{P(x_i,y_j)·log[\frac{P(x_i,y_j)}{P(y_j)}]} \] 
这个量应当理解为你知道Y的值前提下随机变量 X 的随机性的量。&lt;/p&gt;

&lt;h4 id=&quot;134-相对熵&quot;&gt;1.3.4 相对熵&lt;/h4&gt;

&lt;p&gt;       &lt;strong&gt;相对熵（relative entropy）&lt;/strong&gt;又称为&lt;strong&gt;KL散度（Kullback–Leibler divergence，简称KLD）&lt;/strong&gt;，&lt;strong&gt;信息散度（information divergence）&lt;/strong&gt;，&lt;strong&gt;信息增益（information gain）&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;       &lt;code class=&quot;highlighter-rouge&quot;&gt;KL散度&lt;/code&gt;是两个概率分布 &lt;code class=&quot;highlighter-rouge&quot;&gt;P&lt;/code&gt; 和 &lt;code class=&quot;highlighter-rouge&quot;&gt;Q&lt;/code&gt; 差别的非对称性的度量。 &lt;code class=&quot;highlighter-rouge&quot;&gt;KL散度&lt;/code&gt;是用来度量使用基于 &lt;code class=&quot;highlighter-rouge&quot;&gt;Q&lt;/code&gt; 的编码来编码来自 &lt;code class=&quot;highlighter-rouge&quot;&gt;P&lt;/code&gt; 的样本平均所需的额外的位元数。典型情况下，&lt;code class=&quot;highlighter-rouge&quot;&gt;P&lt;/code&gt; 表示数据的真实分布，&lt;code class=&quot;highlighter-rouge&quot;&gt;Q&lt;/code&gt; 表示数据的理论分布，模型分布，或 &lt;code class=&quot;highlighter-rouge&quot;&gt;P&lt;/code&gt; 的近似分布。  对于离散随机变量，其概率分布 &lt;code class=&quot;highlighter-rouge&quot;&gt;P&lt;/code&gt; 和 &lt;code class=&quot;highlighter-rouge&quot;&gt;Q&lt;/code&gt; 的&lt;code class=&quot;highlighter-rouge&quot;&gt;KL散度&lt;/code&gt;可按下式定义为
\[ D_KL(P||Q) = -\sum_{i}{P(i)\frac{Q(i)}{P(i)}} \]
等价于
\[ D_KL(P||Q) = \sum_{i}{P(i)\frac{P(i)}{Q(i)}} \]
即按概率 &lt;code class=&quot;highlighter-rouge&quot;&gt;P&lt;/code&gt; 求得的 &lt;code class=&quot;highlighter-rouge&quot;&gt;P&lt;/code&gt; 和 &lt;code class=&quot;highlighter-rouge&quot;&gt;Q&lt;/code&gt; 的对数差的平均值。&lt;code class=&quot;highlighter-rouge&quot;&gt;KL散度&lt;/code&gt;仅当概率 &lt;code class=&quot;highlighter-rouge&quot;&gt;P&lt;/code&gt; 和 &lt;code class=&quot;highlighter-rouge&quot;&gt;Q&lt;/code&gt; 各自总和均为 &lt;code class=&quot;highlighter-rouge&quot;&gt;1&lt;/code&gt;，且对于任何 &lt;code class=&quot;highlighter-rouge&quot;&gt;i&lt;/code&gt; 皆满足&lt;code class=&quot;highlighter-rouge&quot;&gt;Q(i)&amp;gt;0&lt;/code&gt;及&lt;code class=&quot;highlighter-rouge&quot;&gt;P(i)&amp;gt;0&lt;/code&gt;时，才有定义。式中出现 &lt;code class=&quot;highlighter-rouge&quot;&gt;0ln0&lt;/code&gt; 的情况，其值按0处理。  &lt;br /&gt;
对于连续随机变量，其概率分布 &lt;code class=&quot;highlighter-rouge&quot;&gt;P&lt;/code&gt; 和 &lt;code class=&quot;highlighter-rouge&quot;&gt;Q&lt;/code&gt; 可按积分方式定义为：
\[ D_KL(P||Q) = \int_{-∞}^{∞}{P(i)\frac{Q(i)}{P(i)}}{\rm d}x \] 
尽管从直觉上KL散度是个度量或距离函数, 但是它实际上并不是一个真正的度量或距离。因为KL散度不具有对称性：从分布 &lt;code class=&quot;highlighter-rouge&quot;&gt;P&lt;/code&gt; 到 &lt;code class=&quot;highlighter-rouge&quot;&gt;Q&lt;/code&gt; 的距离通常并不等于从 &lt;code class=&quot;highlighter-rouge&quot;&gt;Q&lt;/code&gt; 到 &lt;code class=&quot;highlighter-rouge&quot;&gt;P&lt;/code&gt; 的距离。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;举个栗子：  &lt;br /&gt;
对于一维数据来说：
&lt;img src=&quot;/assets/tree/tree_3.png&quot; alt=&quot;1&quot; /&gt; 
\[ 对总体的信息熵： ori-Entropy(x) = -\frac{1}{2}log\frac{1}{2} - \frac{1}{2}log\frac{1}{2} = 1 \]
对于&lt;code class=&quot;highlighter-rouge&quot;&gt;A&lt;/code&gt;点来说，左边的信息熵用&lt;code class=&quot;highlighter-rouge&quot;&gt;A_1&lt;/code&gt;表示，右边用&lt;code class=&quot;highlighter-rouge&quot;&gt;A_2&lt;/code&gt;表示，则有
\[ Entropy(A_1) = 0, Entropy(A_2) = -\frac{2}{7}log\frac{2}{7} - \frac{5}{7}log\frac{5}{7} \]
对于信息增益（Information Gain，由于熵的减小，增加信息量的多少）：
\[ IG = ori-Entropy - \sum_{i=1}^n{ω_i}{Entropy(A_i)} \]
即：
\[ IG = Entropy(A) - \sum_{i=1}^n{\frac{\lvert{A_i}\rvert}{A}}{Entropy(A_i)} \] 
所以有 &lt;code class=&quot;highlighter-rouge&quot;&gt;A&lt;/code&gt; 点处的信息增益为：
\[ IG(A) = 1 - \frac{3}{10}×0 + \frac{7}{10}×Entropy(A_2) \]
所以， &lt;code class=&quot;highlighter-rouge&quot;&gt;B&lt;/code&gt; 点处的信息增益为：
\[ IG(B) = 1 - \frac{6}{10}×Entropy(B_1) + \frac{4}{10}×Entropy(B_2) \]
同理可求出&lt;code class=&quot;highlighter-rouge&quot;&gt;IG(C)&lt;/code&gt;，选择&lt;code class=&quot;highlighter-rouge&quot;&gt;IG(A)&lt;/code&gt;,&lt;code class=&quot;highlighter-rouge&quot;&gt;IG(B)&lt;/code&gt;,&lt;code class=&quot;highlighter-rouge&quot;&gt;IG(C)&lt;/code&gt;中的最大值就是划分的最佳结点。  &lt;br /&gt;
再比如上回提到的 Play Tennis 的预测模型：
&lt;img src=&quot;/assets/tree/tree_4.png&quot; alt=&quot;1&quot; /&gt;
以上的步骤就是&lt;code class=&quot;highlighter-rouge&quot;&gt;ID3算法&lt;/code&gt;（根据信息增益，确定合适的节点）的基本思路，实际就是将空间分为很多区域，根据特征类型进行划分，我们可以将空间划分为：&lt;code class=&quot;highlighter-rouge&quot;&gt;ordinal(有序的)&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;numerial(数字的)&lt;/code&gt;,  &lt;code class=&quot;highlighter-rouge&quot;&gt;discrete(离散的)&lt;/code&gt;。对于&lt;strong&gt;子树&lt;/strong&gt;(sub-Trees)的选择，取决于两个属性：&lt;code class=&quot;highlighter-rouge&quot;&gt;attribute types&lt;/code&gt;(Nominal, Ordinal, Continuous), &lt;code class=&quot;highlighter-rouge&quot;&gt;number of ways to split&lt;/code&gt;(2-way split, Muti-way split)。对于&lt;strong&gt;连续数据的离散化&lt;/strong&gt;（Discretization）可以根据&lt;code class=&quot;highlighter-rouge&quot;&gt;数据分布&lt;/code&gt;，&lt;code class=&quot;highlighter-rouge&quot;&gt;区域&lt;/code&gt;，&lt;code class=&quot;highlighter-rouge&quot;&gt;熵&lt;/code&gt;进行离散化。对于&lt;strong&gt;分割&lt;/strong&gt;（splitting），有 &lt;code class=&quot;highlighter-rouge&quot;&gt;Muti-way split&lt;/code&gt;(Use as many partitions as distinct values.) 和 &lt;code class=&quot;highlighter-rouge&quot;&gt;Binary split&lt;/code&gt;(Divide values into two subject. Need to find optional partitioning.) 两种。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;14-信息增益比&quot;&gt;1.4 信息增益比&lt;/h3&gt;

&lt;p&gt;       以信息增益比作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。使用&lt;strong&gt;信息增益比&lt;/strong&gt;（Information gain ratio）可以对这一问题进行校正，这是特征选择的标准之一。  &lt;br /&gt;
       特征 &lt;code class=&quot;highlighter-rouge&quot;&gt;A&lt;/code&gt; 对数据集 &lt;code class=&quot;highlighter-rouge&quot;&gt;D&lt;/code&gt; 的信息增益比&lt;code class=&quot;highlighter-rouge&quot;&gt;g_r(D,A)&lt;/code&gt;定义为信息增益&lt;code class=&quot;highlighter-rouge&quot;&gt;g(D,A)&lt;/code&gt;与训练数据集 &lt;code class=&quot;highlighter-rouge&quot;&gt;D&lt;/code&gt; 关于特征 &lt;code class=&quot;highlighter-rouge&quot;&gt;A&lt;/code&gt; 的值的熵&lt;code class=&quot;highlighter-rouge&quot;&gt;H_A(D)&lt;/code&gt;之比，即 
\[ g_R(D,A) = \frac{g(D,A)}{H_A(D)} \] 
其中，
\[ -\sum_{i=1}^n{\frac{\lvert{D_i}\rvert}{\lvert{D}\rvert}{log}_2}{\frac{\lvert{D_i}\rvert}{\lvert{D}\rvert}} \] 
&lt;code class=&quot;highlighter-rouge&quot;&gt;n&lt;/code&gt; 是特征 &lt;code class=&quot;highlighter-rouge&quot;&gt;A&lt;/code&gt; 取值的个数。&lt;/p&gt;

&lt;h3 id=&quot;14-gini-split&quot;&gt;1.4 Gini Split&lt;/h3&gt;

&lt;p&gt;       对于给定样本集合 &lt;code class=&quot;highlighter-rouge&quot;&gt;D&lt;/code&gt;，其&lt;code class=&quot;highlighter-rouge&quot;&gt;GINI Index&lt;/code&gt;（基尼系数）为：
\[ GINI(D) = 1 - \sum_{k=1}^K{[\frac{\lvert{C_k}\rvert}{\lvert{D}\rvert}]}^2 \] 
其中 &lt;code class=&quot;highlighter-rouge&quot;&gt;C_k&lt;/code&gt; 是 &lt;code class=&quot;highlighter-rouge&quot;&gt;D&lt;/code&gt; 中属于第 &lt;code class=&quot;highlighter-rouge&quot;&gt;k&lt;/code&gt; 类的样本子集, &lt;code class=&quot;highlighter-rouge&quot;&gt;K&lt;/code&gt; 是类的个数。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;举个栗子:
如果中有 &lt;code class=&quot;highlighter-rouge&quot;&gt;C_1&lt;/code&gt; 类 &lt;code class=&quot;highlighter-rouge&quot;&gt;0&lt;/code&gt; 个，&lt;code class=&quot;highlighter-rouge&quot;&gt;C_2&lt;/code&gt; 类 &lt;code class=&quot;highlighter-rouge&quot;&gt;6&lt;/code&gt; 个，则&lt;code class=&quot;highlighter-rouge&quot;&gt;GINI&lt;/code&gt;系数为：
\[ GINI(C) = 1 - \sum_{i=1}^2{[\frac{\lvert{C_i}\rvert}{\lvert{D}\rvert}]}^2 = 1 - [[\frac{0}{6}]^2 + [\frac{6}{6}]^2] = 1 \] 
如果中有 &lt;code class=&quot;highlighter-rouge&quot;&gt;C_1&lt;/code&gt; 类 &lt;code class=&quot;highlighter-rouge&quot;&gt;3&lt;/code&gt; 个，&lt;code class=&quot;highlighter-rouge&quot;&gt;C_2&lt;/code&gt; 类 &lt;code class=&quot;highlighter-rouge&quot;&gt;3&lt;/code&gt; 个，则&lt;code class=&quot;highlighter-rouge&quot;&gt;GINI&lt;/code&gt;系数为：
\[ GINI(C) = 1 - \sum_{i=1}^2{[\frac{\lvert{C_i}\rvert}{\lvert{D}\rvert}]}^2 = 1 - [[\frac{3}{6}]^2 + [\frac{3}{6}]^2] = \frac{1}{2} \]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;基尼系数具有以下特点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;总体内包含的类别越杂乱，GINI指数就越大；&lt;/li&gt;
  &lt;li&gt;类别个数越少，基尼系数越低；   &lt;/li&gt;
  &lt;li&gt;类别个数相同时，类别集中度越高，基尼系数越低；&lt;/li&gt;
  &lt;li&gt;当类别越少，类别集中度越高的时候，基尼系数越低；当类别越多，类别集中度越低的时候，基尼系数越高。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;GINI Split&lt;/strong&gt;：&lt;/p&gt;

&lt;p&gt;Used in CART, SLIQ, SPRINT.  &lt;br /&gt;
When a node p is split into k partitions (children), the quality of split is computed as,  &lt;br /&gt;
\[
 GINI_(split) = 1 - \sum_{i=1}^{k} {\frac{n_i}{n}GINI(i)} 
\] 
where, n_i = number of records at child i, n = number of records at node p.&lt;/p&gt;

&lt;p&gt;For efficient computation:   &lt;br /&gt;
for each attribute,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Sort the attribute on values;&lt;/li&gt;
  &lt;li&gt;Linearly scan these values, each time updating the count matrix and computing gini index;&lt;/li&gt;
  &lt;li&gt;Choose the split position that has the least gini index.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For an example:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/tree/tree_5.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;基尼指数的基本思想是：对每个属性都遍历所有的分割方法后若能提供最小的 GINI_Split，就被选择作为此节点处分裂的标准，无论处于根节点还是子节点。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;15-misclassification-error&quot;&gt;1.5 Misclassification Error&lt;/h3&gt;

&lt;p&gt;\[ Error(t) = 1 - max P(i|t) \]
举个栗子:
&lt;img src=&quot;/assets/tree/tree_7.png&quot; style=&quot;zoom:50%&quot; /&gt;
Measure of Impurity for 2-Class Problems:
&lt;img src=&quot;/assets/tree/tree_6.png&quot; style=&quot;zoom:50%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;二-id3-和-c45-算法&quot;&gt;二. ID3 和 C4.5 算法&lt;/h2&gt;

&lt;p&gt;       机器学习中，决策树是一个预测模型；他代表的是对象属性与对象值之间的一种映射关系。树中每个节点表示某个对象，而每个分叉路径则代表的某个可能的属性值，而每个叶结点则对应从根节点到该叶节点所经历的路径所表示的对象的值。决策树仅有单一输出，若欲有复数输出，可以建立独立的决策树以处理不同输出。从数据产生决策树的机器学习技术叫做&lt;strong&gt;决策树学习&lt;/strong&gt;,通俗说就是&lt;strong&gt;决策树&lt;/strong&gt;。决策树学习也是数据挖掘中一个普通的方法。在这里，每个决策树都表述了一种树型结构，他由他的分支来对该类型的对象依靠属性进行分类。每个决策树可以依靠对源数据库的分割进行数据测试。这个过程可以递归式的对树进行修剪。当不能再进行分割或一个单独的类可以被应用于某一分支时，递归过程就完成了。另外，随机森林分类器将许多决策树结合起来以提升分类的正确率。决策树同时也可以依靠计算条件概率来构造。决策树如果依靠数学的计算方法可以取得更加理想的效果。  &lt;br /&gt;
 &lt;strong&gt;决策树是如何工作的？&lt;/strong&gt;  &lt;br /&gt;
 决策树一般都是自上而下的来生成的。选择分割的方法有好几种，但是目的都是一致的：对目标类尝试进行最佳的分割。从根到叶子节点都有一条路径，这条路径就是一条“规则”。决策树可以是二叉的，也可以是多叉的。对每个节点的衡量:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;通过该节点的记录数&lt;/li&gt;
  &lt;li&gt;如果是叶子节点的话，分类的路径&lt;/li&gt;
  &lt;li&gt;对叶子节点正确分类的比例。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;有些规则的效果可以比其他的一些规则要好。由于 &lt;code class=&quot;highlighter-rouge&quot;&gt;ID3&lt;/code&gt; 算法在实际应用中存在一些问题，于是 &lt;code class=&quot;highlighter-rouge&quot;&gt;Quilan&lt;/code&gt; 提出了 &lt;code class=&quot;highlighter-rouge&quot;&gt;C4.5&lt;/code&gt; 算法，严格上说 &lt;code class=&quot;highlighter-rouge&quot;&gt;C4.5&lt;/code&gt; 只能是 &lt;code class=&quot;highlighter-rouge&quot;&gt;ID3&lt;/code&gt; 的一个改进算法。下面对两个算法做详细的说明。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;特征选择&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;特征选择指选择最大化所定义目标函数的特征。下面给出如下三种特征（Gender, Car Type, Customer ID）分裂的例子：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/tree/tree_12.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;图中有两类类别&lt;code class=&quot;highlighter-rouge&quot;&gt;（C0, C1）&lt;/code&gt;，&lt;code class=&quot;highlighter-rouge&quot;&gt;C0: 6&lt;/code&gt; 是对 &lt;code class=&quot;highlighter-rouge&quot;&gt;C0&lt;/code&gt; 类别的计数。直观上，应选择 &lt;code class=&quot;highlighter-rouge&quot;&gt;Car Type&lt;/code&gt; 特征进行分裂，因为其类别的分布概率具有更大的倾斜程度，类别不确定程度更小。
为了衡量类别分布概率的倾斜程度，定义决策树节点 &lt;code class=&quot;highlighter-rouge&quot;&gt;t&lt;/code&gt; 的不纯度（impurity），其满足：不纯度越小，则类别的分布概率越倾斜；下面给出不纯度的的三种度量：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/tree/tree_13.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;其中，$ p(c_k|t) $ 表示对于决策树节点 &lt;code class=&quot;highlighter-rouge&quot;&gt;t&lt;/code&gt; 类别 &lt;code class=&quot;highlighter-rouge&quot;&gt;c_k&lt;/code&gt; 的概率。这三种不纯度的度量是等价的，在等概率分布是达到最大值。
为了判断分裂前后节点不纯度的变化情况，目标函数定义为信息增益（information gain）：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/tree/tree_14.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;I(⋅)&lt;/code&gt; 对应于决策树节点的不纯度，&lt;code class=&quot;highlighter-rouge&quot;&gt;parent&lt;/code&gt; 表示分裂前的父节点，&lt;code class=&quot;highlighter-rouge&quot;&gt;N&lt;/code&gt; 表示父节点所包含的样本记录数，$ a_i $ 表示父节点分裂后的某子节点，$ N(a_i) $ 为其计数，&lt;code class=&quot;highlighter-rouge&quot;&gt;n&lt;/code&gt; 为分裂后的子节点数。&lt;/p&gt;

&lt;p&gt;特别地，ID3算法选取熵值作为不纯度 &lt;code class=&quot;highlighter-rouge&quot;&gt;I(⋅)&lt;/code&gt; 的度量，则&lt;/p&gt;

&lt;p&gt;\[ 
\Delta = H(c) - \sum_{i=1}^{n} {\frac{N(a_i)}{N}H(c|a_i)}
 = H(c) - \sum_{i=1}^{n} {p(a_i)H(c|a_i)}
 = H(c) - H(c|A) 
 \]&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;c&lt;/code&gt; 指父节点对应所有样本记录的类别；&lt;code class=&quot;highlighter-rouge&quot;&gt;A&lt;/code&gt; 表示选择的特征属性，即 $ a_i $ 的集合。那么，决策树学习中的信息增益 &lt;code class=&quot;highlighter-rouge&quot;&gt;Δ&lt;/code&gt; 等价于训练数据集中&lt;strong&gt;类与特征的互信息&lt;/strong&gt;，表示由于得知特征 &lt;code class=&quot;highlighter-rouge&quot;&gt;A&lt;/code&gt; 的信息训练数据集 &lt;code class=&quot;highlighter-rouge&quot;&gt;c&lt;/code&gt; 不确定性减少的程度。&lt;/p&gt;

&lt;p&gt;       在特征分裂后，有些子节点的记录数可能偏少，以至于影响分类结果。为了解决这个问题，CART算法提出了只进行特征的二元分裂，即决策树是一棵二叉树；C4.5算法改进分裂目标函数，用信息增益比（information gain ratio）来选择特征：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/tree/tree_14.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;       因而，特征选择的过程等同于计算每个特征的信息增益，选择最大信息增益的特征进行分裂。ID3算法设定一阈值，当最大信息增益小于阈值时，认为没有找到有较优分类能力的特征，没有往下继续分裂的必要。根据最大表决原则，将最多计数的类别作为此叶子节点。即回答前面所提出的第二个问题（停止分裂条件）。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;决策树生成&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;ID3算法的核心是根据信息增益最大的准则，递归地构造决策树；算法流程如下：&lt;/p&gt;
&lt;blockquote&gt;

  &lt;p&gt;1 如果节点满足停止分裂条件（所有记录属同一类别 or 最大信息增益小于阈值），将其置为叶子节点；&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;2 选择信息增益最大的特征进行分裂；&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;3 重复步骤1-2，直至分类完成。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;C4.5算法流程与ID3相类似，只不过将信息增益改为信息增益比。&lt;/p&gt;

&lt;h3 id=&quot;21-id3-算法&quot;&gt;2.1 ID3 算法&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;构建决策树需要解决以下三个问题:
（1）数据是怎么分裂的
（2）如何选择分类的属性
（3）什么时候停止分裂&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ID3 算法过程如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/tree/tree_8.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;从根结点开始，对结点计算所有可能的特征的&lt;strong&gt;信息增益&lt;/strong&gt;，选择&lt;strong&gt;信息增益&lt;/strong&gt;最大的特征作为结点的特征，由该特征的不同取值来建立子结点；再对子结点递归的调用以上方法，构建决策树；直到所有特征的&lt;strong&gt;信息增益&lt;/strong&gt;均很小或没有特征可以选择为止。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt; &lt;strong&gt;ID3 算法对数据的要求:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;所有属性必须为离散量。&lt;/li&gt;
  &lt;li&gt;所有的训练例的所有属性必须有一个明确的值。&lt;/li&gt;
  &lt;li&gt;相同的因素必须得到相同的结论且训练例必须唯一。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;22-c45-算法&quot;&gt;2.2 C4.5 算法&lt;/h3&gt;

&lt;p&gt;信息增益比的形式来选择特征，这样的算法叫做 &lt;code class=&quot;highlighter-rouge&quot;&gt;C4.5&lt;/code&gt; 算法。
&lt;code class=&quot;highlighter-rouge&quot;&gt;C4.5&lt;/code&gt; 算法过程如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/tree/tree_9.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;C4.5 算法有如下&lt;strong&gt;优点&lt;/strong&gt;:  &lt;br /&gt;
       产生的分类规则易于理解，准确率较高。  &lt;br /&gt;
其&lt;strong&gt;缺点&lt;/strong&gt;是:  &lt;br /&gt;
       在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效。  &lt;br /&gt;
       此外，&lt;code class=&quot;highlighter-rouge&quot;&gt;C4.5&lt;/code&gt; 只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时程序无法运行。  &lt;br /&gt;
       来自搜索的其他内容：&lt;code class=&quot;highlighter-rouge&quot;&gt;C4.5&lt;/code&gt; 算法是机器学习算法中的一种分类决策树算法,其核心算法是 &lt;code class=&quot;highlighter-rouge&quot;&gt;ID3&lt;/code&gt; 算法.&lt;/p&gt;

&lt;h3 id=&quot;23-id3-和-c45-的对比&quot;&gt;2.3 ID3 和 C4.5 的对比&lt;/h3&gt;

&lt;p&gt;C4.5 算法继承了 ID3 算法的优点，并在以下几方面对 ID3 算法进行了改进:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;用信息增益率来选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;在树构造过程中进行剪枝;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;能够完成对连续属性的离散化处理;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;能够对不完整数据进行处理。     &lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;23-决策树剪枝&quot;&gt;2.3 决策树剪枝&lt;/h3&gt;

&lt;h4 id=&quot;231-过拟合&quot;&gt;2.3.1 过拟合&lt;/h4&gt;

&lt;p&gt;       生成的决策树对训练数据会有很好的分类效果，却可能对未知数据的预测不准确，即决策树模型发生过拟合（overfitting）—— 训练误差（training error）很小、泛化误差（generalization error，亦可看作为test error）较大。下图给出训练误差、测试误差（test error）随决策树节点数的变化情况：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/tree/tree_16.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;       可以观察到，当节点数较小时，训练误差与测试误差均较大，即发生了&lt;strong&gt;欠拟合（underfitting）&lt;/strong&gt;。当节点数较大时，训练误差较小，测试误差却很大，即发生了&lt;strong&gt;过拟合&lt;/strong&gt;。只有当节点数适中是，训练误差居中，测试误差较小；对训练数据有较好的拟合，同时对未知数据有很好的分类准确率。&lt;/p&gt;

&lt;p&gt;发生过拟合的根本原因是分类模型过于复杂，可能的原因如下：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;训练数据集中有噪音样本点，对训练数据拟合的同时也对噪音进行拟合，从而影响了分类的效果；&lt;/li&gt;
  &lt;li&gt;决策树的叶子节点中缺乏有分类价值的样本记录，也就是说此叶子节点应被剪掉。&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;232-剪枝策略&quot;&gt;2.3.2 剪枝策略&lt;/h4&gt;

&lt;p&gt;为了解决过拟合，&lt;code class=&quot;highlighter-rouge&quot;&gt;C4.5&lt;/code&gt; 通过剪枝以减少模型的复杂度。一种简单剪枝策略，通过极小化决策树的整体损失函数（loss function）或代价函数（cost function）来实现，决策树 &lt;code class=&quot;highlighter-rouge&quot;&gt;T&lt;/code&gt; 的损失函数为
\[ L_\alpha(T) = C(T) + \alpha |T| \]
其中，&lt;code class=&quot;highlighter-rouge&quot;&gt;C(T)&lt;/code&gt; 表示决策树的训练误差，&lt;code class=&quot;highlighter-rouge&quot;&gt;α&lt;/code&gt; 为调节参数，&lt;code class=&quot;highlighter-rouge&quot;&gt;|T|&lt;/code&gt; 为模型的复杂度。当模型越复杂时，训练的误差就越小。上述定义的损失正好做了两者之间的权衡。&lt;/p&gt;

&lt;p&gt;如果剪枝后损失函数减少了，即说明这是有效剪枝。具体剪枝算法可以由动态规划等来实现。&lt;/p&gt;

&lt;p&gt;决策树剪枝算法过程如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/tree/tree_11.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;总结一下：  &lt;br /&gt;
由完全树 $T_0$ 开始，剪枝部分结点得到 $T_1$，再次剪枝部分结点得到 $T_2$，直到剪枝到仅剩下树根的那棵树 $T_k$。当然这些树都要保留 ${T_1,T_2,….,T_k}$；  &lt;br /&gt;
接着通过交叉验证法在验证数据集上对这 &lt;code class=&quot;highlighter-rouge&quot;&gt;k&lt;/code&gt; 棵树分别进行测试评价，选择损失函数最小的数 $T_α$ 作为最优子树。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;三-tree-ensembles---cart&quot;&gt;三. Tree Ensembles - CART&lt;/h2&gt;

&lt;p&gt;CART 算法过程如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/tree/tree_10.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;四-tree-ensembles---bagging&quot;&gt;四. Tree Ensembles - Bagging&lt;/h2&gt;

&lt;p&gt;Bagging策略(bootstrap aggregating)套袋法&lt;/p&gt;

&lt;p&gt;从N个样本中有放回的随机抽取n个样本。
用这n个样本的所有属性构建基分类器（LR,ID3,C4.5,SVM）
重复1,2两步m次，构建m个基分类器
投票得出分类结果，哪个票最多就是哪一类。对回归模型，取多有基分类器结果的均值。总而言之就是所有基分类器的权重相同。&lt;/p&gt;

&lt;h2 id=&quot;五-tree-ensembles---random-forest&quot;&gt;五. Tree Ensembles - Random Forest&lt;/h2&gt;

&lt;p&gt;随机森林&lt;/p&gt;

&lt;p&gt;bagging方法可以有效降低模型的方差。随机森林每棵子树不需要剪枝，是低偏差高方差的模型，通过bagging降低方差后使得整个模型有较高的性能。&lt;/p&gt;

&lt;p&gt;随机森林其实很简单，就是在bagging策略上略微改动了一下。&lt;/p&gt;

&lt;p&gt;从N个样本中有放回的随机抽样n个样本。
如果每个样本的特征维度为M，指定一个常数m«M，随机地从M个特征中选取m个特征子集，每次树（ID3,C4.5,CART）进行分裂时，从这m个特征中选择最优的(信息增益，信息增益比率，基尼系数)；
每棵树都尽最大程度的生长，并且没有剪枝过程。
最后采用投票表决的方式进行分类。
特征m个数的选取：&lt;/p&gt;

&lt;p&gt;用作分类时，m默认取，最小取1.&lt;/p&gt;

&lt;p&gt;用作回归时，m默认取M/3，最小取5.&lt;/p&gt;

&lt;p&gt;两个随机性的引入对随机森林的分类性能至关重要。由于它们的引入，使得随机森林不容易陷入过拟合，并且具有很好得抗噪能力（比如：对缺省值不敏感）。&lt;/p&gt;

&lt;p&gt;随机森林分类效果（错误率）与两个因素有关：&lt;/p&gt;

&lt;p&gt;森林中任意两棵树的相关性：相关性越大，错误率越大；
森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低。
　　减小特征选择个数m，树的相关性和分类能力也会相应的降低；增大m，两者也会随之增大。所以关键问题是如何选择最优的m（或者是范围），这也是随机森林唯一的一个参数。&lt;/p&gt;

&lt;h2 id=&quot;六-tree-ensembles---boosting&quot;&gt;六. Tree Ensembles - Boosting&lt;/h2&gt;

&lt;p&gt;Bootstrapping&lt;/p&gt;

&lt;p&gt;Bootstrapping从字面意思翻译是拔靴法，从其内容翻译又叫自助法，是一种再抽样的统计方法。自助法的名称来源于英文短语“to pull oneself up by one’s bootstrap”，表示完成一件不能自然完成的事情。1977年美国Standford大学统计学教授Efron提出了一种新的增广样本的统计方法，就是Bootstrap方法，为解决小子样试验评估问题提供了很好的思路。&lt;/p&gt;

&lt;p&gt;算法流程&lt;/p&gt;

&lt;p&gt;从N个样本中有放回的随机抽取n个样本。
用n个样本计算统计量
重复1，2步骤m次，得到统计量
计算统计量序列的方差，则可得到统计量方差的无偏估计。（均值也是，bootstrapping方法就是通过多次的随机采样得到可以代表母体样本分布的子样本分布）&lt;/p&gt;

&lt;p&gt;Boosting
　其主要思想是将弱分类器组装成强分类器。在PAC(概率近似正确)的框架下，则一定可以将一个弱分类器组装成一个强分类器。
　关于 Boosting 的两个核心问题：
　1. 在每一轮如何改变训练数据的权值或概率分布？
通过提高前一轮中被弱分类器分错样例的权值，减少前一轮对样例的权值，来使分类区对误分的数据有较好的效果。
　
　2. 通过什么方式在组合弱分类器？　
　　通过加法模型将弱分类器线性组合，比如Aadaboost 通过加权多次表决的方式，即增大错误率小的分类器的权值，同时减小错误率大的分类器的权值。而提升树通过拟合残差的方式逐步减小误差，将每一步生成模型叠加得到最终模型。
　　
　　Bagging 和 Boosting 二者之间的区别
　　1. 样本选择上
Bagging: 训练集在样本集中有放回选取，从原实际中选出的各轮训练集之间是独立的。
Boosting: 每一轮的训练集不变，只是训练集中每一个样例在分类器中的权重发生变化。而权重是根据上一轮的分类结果进行调整的。
　　
　　2. 样例权重
Bagging: 使用均匀取样，每个样例权重相等。
Boosting: 根据错误率不断调整样本的权值，错误率却大则权重越大。
　　
　　3. 预测函数
Bagging: 使所有预测函数的权重相等。
Boosting: 每个弱分类器都有相应的权重，对于分类误差小的分类器，会有更大的权重。
　　
　　4. 并行计算
Bagging: 各个误差函数可以并行生成。
Boosting: 每个误差函数只能顺序生成，因为后一个模型参数需要前一轮模型结果。&lt;/p&gt;

&lt;h2 id=&quot;七-tree-ensembles---gradient-boosting-decision-treegbdt&quot;&gt;七. Tree Ensembles - Gradient Boosting Decision Tree(GBDT)&lt;/h2&gt;

&lt;h2 id=&quot;八-参考文献&quot;&gt;八. 参考文献&lt;/h2&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;《 决策树中基于基尼指数的属性分裂方法 》 陈云樱等
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;　　OOB（Out Of Bag）袋外错误率&lt;/p&gt;

&lt;p&gt;在bootstrapping的过程中，有些数据可能没有被选择，这些数据称为out-of-bag(OOB) examples。&lt;/p&gt;

&lt;p&gt;解释一下上面这张图。一眼看还是挺难理解的，用白话讲一下。&lt;/p&gt;

&lt;p&gt;是什么？&lt;/p&gt;

&lt;p&gt;随机森林中每一次样本抽样（不是特征抽样），就是bootstrapping方法，是有放回的随机抽样，所以每一次抽的时候，对于一个特定的样本，抽到它的概率就是，很好理解，N个样本里随机抽取一个，抽到的概率当然是，因为是有放回的抽样，所以分母永远是N。&lt;/p&gt;

&lt;p&gt;是什么？&lt;/p&gt;

&lt;p&gt;既然被抽到的概率是，那不被抽到的概率就是,很好理解。那指数大N又是什么呢？其实就是抽样的次数。假设我们的随机森林一共有A颗树，每棵树抽了B个样本，那么指数大N就是，是不是感觉指数大N应该和分母的那个N一样？其实这里只是为了方便，它表达的意思就是抽了很多次。&lt;/p&gt;

&lt;p&gt;搞明白了那个公式之后，就可以开始计算了。要用到数学分析中的极限知识。一步步推一下。&lt;/p&gt;

&lt;p&gt;这是基本公式： 后面的都是基于这个变形&lt;/p&gt;

&lt;p&gt;这说明了什么呢？就是你随机森林已经造好了，但是即使你的训练集是整个样本集，其中也会有的样本你抽不到。为什么抽不到，上面的公式就是告诉你为什么抽不到。这些抽不到的样本就叫做out-of-bag(OOB) examples&lt;/p&gt;

&lt;p&gt;好了，到这里已经能搞懂什么是out-of-bag(OOB) examples了。那这些样本能用来做什么呢？下面就介绍oob袋外错误率。&lt;/p&gt;

&lt;p&gt;袋外错误率的应用&lt;/p&gt;

&lt;p&gt;正常情况下，我们训练一个模型，怎么验证它好不好。是不是要拿出一部分的数据集当作验证集，更好的还要拿出一部分当作测试集，一般是6:2:2。&lt;/p&gt;

&lt;p&gt;在随机森林中，有了out-of-bag(OOB) examples，我们就不需要拿出一部分数据了，out-of-bag(OOB) examples就是那部分没有用到的数据，我们可以直接当成验证集来使用。&lt;/p&gt;

&lt;p&gt;obb error = 被分类错误数 / 总数&lt;/p&gt;

&lt;p&gt;随机森林算法中不需要再进行交叉验证或者单独的测试集来获取测试集误差的无偏估计。&lt;/p&gt;

&lt;p&gt;Breiman [1996b]在对 bagged 分类器的错误率估计研究中, 给出实证证据显示，out-of-bag 估计 和使用与训练集大小一致的测试集所得到的错误率一样精确. 所以, 使用out-of-bag error 估计可以不在另外建立一个测试集.&lt;/p&gt;

&lt;p&gt;特征重要性度量&lt;/p&gt;

&lt;p&gt;计算某个特征X的重要性时，具体步骤如下：&lt;/p&gt;

&lt;p&gt;对整个随机森林，得到相应的袋外数据（out of bag，OOB）​计算袋外数据误差，记为errOOB1.&lt;/p&gt;

&lt;p&gt;所谓袋外数据是指，每次建立决策树时，通过重复抽样得到一个数据用于训练​决策树，这时还有大约1/3的数据没有被利用，没有参与决策树的建立。这部分数据可以用于对决策树的性能进行评估，计算模型的预测错误率，称为袋外数据误差。&lt;/p&gt;

&lt;p&gt;​这已经经过证明是无偏估计的,所以在随机森林算法中不需要再进行交叉验证或者单独的测试集来获取测试集误差的无偏估计。&lt;/p&gt;

&lt;p&gt;随机对袋外数据OOB所有样本的特征X加入噪声干扰（可以随机改变样本在特征X处的值），再次计算袋外数据误差，记为errOOB2。&lt;/p&gt;

&lt;p&gt;​假设森林中有N棵树，则特征X的重要性=∑errOOB2−errOOB1N∑errOOB2−errOOB1N。这个数值之所以能够说明特征的重要性是因为，如果加入随机噪声后，袋外数据准确率大幅度下降（即errOOB2上升），说明这个特征对于样本的预测结果有很大影响，进而说明重要程度比较高。
特征选择&lt;/p&gt;

&lt;p&gt;在特征重要性的基础上，特征选择的步骤如下：&lt;/p&gt;

&lt;p&gt;计算每个特征的重要性，并按降序排序
确定要剔除的比例，依据特征重要性剔除相应比例的特征，得到一个新的特征集
用新的特征集重复上述过程，直到剩下m个特征（m为提前设定的值）。
根据上述过程中得到的各个特征集和特征集对应的袋外误差率，选择袋外误差率最低的特征集。​
优点&lt;/p&gt;

&lt;p&gt;在数据集上表现良好
在当前的很多数据集上，相对其他算法有着很大的优势
它能够处理很高维度（feature很多）的数据，并且不用做特征选择
在训练完后，它能够给出哪些feature比较重要
在创建随机森林的时候，对generlization error使用的是无偏估计
训练速度快
在训练过程中，能够检测到feature间的互相影响
容易做成并行化方法
实现比较简单
可以得到变量重要性排序（两种：基于OOB误分率的增加量和基于分裂时的GINI下降量）
随机森林有许多优点：&lt;/p&gt;

&lt;p&gt;具有极高的准确率
随机性的引入，使得随机森林不容易过拟合
随机性的引入，使得随机森林有很好的抗噪声能力
能处理很高维度的数据，并且不用做特征选择
既能处理离散型数据，也能处理连续型数据，数据集无需规范化
训练速度快，可以得到变量重要性排序
容易实现并行化
随机森林的缺点：&lt;/p&gt;

&lt;p&gt;当随机森林中的决策树个数很多时，训练时需要的空间和时间会较大
随机森林模型还有许多不好解释的地方，有点算个黑盒模型&lt;/p&gt;</content><author><name>被水淹死的鱼</name></author><category term="机器学习" /><summary type="html">[TOC]</summary></entry><entry><title type="html">自然语言处理综述</title><link href="http://localhost:4000/2018/10/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9C%A8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8%E7%BB%BC%E8%BF%B0.html" rel="alternate" type="text/html" title="自然语言处理综述" /><published>2018-10-26T00:00:00+08:00</published><updated>2018-10-26T00:00:00+08:00</updated><id>http://localhost:4000/2018/10/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9C%A8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8%E7%BB%BC%E8%BF%B0</id><content type="html" xml:base="http://localhost:4000/2018/10/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9C%A8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8%E7%BB%BC%E8%BF%B0.html">&lt;h1 id=&quot;深度学习在自然语言处理的发展与应用&quot;&gt;深度学习在自然语言处理的发展与应用&lt;/h1&gt;

&lt;h2 id=&quot;摘要&quot;&gt;摘要&lt;/h2&gt;
&lt;p&gt;       深度学习的概念源于人工神经网络的研究。含多隐层的多层感知器就是一种深度学习结构。深度学习通过组合低层特征形成更加抽象的高层表示属性类别或特征，以发现数据的分布式特征表示。深度学习是机器学习研究中的一个新的领域，其动机在于建立、模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，例如图像，声音和文本。
       在本文中，我描述了可以使用神经网络解决的自然语言处理问题。并讨论了相关技术发展及少量技术细节。神经网络可以用于文本分类，信息提取，语义解析，问答，释义检测，语言生成，多文档分类，机器翻译，语音识别等诸多领域。在许多情况下，神经网络方法优于其他方法。&lt;/p&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;

&lt;p&gt;       The concept of deep learning stems from the study of artificial neural networks. A multilayer perceptron with multiple hidden layers is a deep learning structure. Deep learning combines low-level features to form more abstract high-level representation attribute categories or features to discover distributed feature representations of data. Deep learning is a new field in machine learning research. Its motivation is to build and simulate a neural network for human brain analysis and learning. It mimics the mechanism of the human brain to interpret data such as images, sounds and texts.&lt;/p&gt;

&lt;p&gt;       In this article, I describe natural language processing problems that can be solved using neural networks. And discussed the development of related technologies and a small amount of technical details. Neural networks can be used for text classification, information extraction, semantic analysis, question and answer, interpretation detection, language generation, multi-document classification, machine translation, speech recognition and many other fields. In many cases, neural network methods are superior to other methods.&lt;/p&gt;

&lt;h3 id=&quot;概述&quot;&gt;概述&lt;/h3&gt;
&lt;p&gt;深度机器学习方法分有监督学习与无监督学习．不同的学习框架下建立的学习模型很是不同．例如，卷积神经网络（Convolutional neural networks，简称CNNs）就是一种深度的监督学习下的机器学习模型，而深度置信网（Deep Belief Nets，简称DBNs）就是一种无监督学习下的机器学习模型。本文我主要描述了可以使用神经网络解决的自然语言处理问题。并讨论了相关技术发展及少量技术细节。神经网络可以用于文本分类，信息提取，语义解析，问答，释义检测，语言生成，多文档分类，机器翻译，语音识别等诸多领域。在许多情况下，神经网络方法优于其他方法。&lt;/p&gt;

&lt;h3 id=&quot;正文&quot;&gt;正文&lt;/h3&gt;

&lt;p&gt;       由于人工神经网络可以对非线性过程进行建模，因此已经成为解决诸如分类，聚类，回归，模式识别，维度简化，结构化预测，机器翻译，异常检测，决策可视化，计算机视觉和其他许多问题的利器。这种广泛的能力使得人工神经网络可以应用于许多领域。我在这里简单地讨论人工神经网络在自然语言处理任务（NLP）中的应用。&lt;/p&gt;

&lt;p&gt;       NLP包括广泛的语法，语义，会话和语音等任务。我们将主要描述神经网络取得优异成绩的一些领域：&lt;/p&gt;

&lt;h4 id=&quot;1信息抽取&quot;&gt;1.信息抽取&lt;/h4&gt;
&lt;p&gt;       信息抽取的主要任务是从非结构化文档自动导出结构化信息。该任务包括许多子任务，如命名实体识别，一致性解析，关系抽取，术语抽取等。
       命名实体识别（NER）的主要任务是将诸如Microsoft，London等的命名实体分类为人员，组织，地点，时间，日期等预定类别。许多NER系统已经创建，其中最好系统采用的是神经网络。
       NER一直是NLP领域中的研究热点，从早期基于词典和规则的方法，到传统机器学习的方法，到近年来基于深度学习的方法，NER研究进展的大概趋势大致如下图所示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/summary/summary_1.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;       在基于机器学习的方法中，NER被当作序列标注问题。利用大规模语料来学习出标注模型，从而对句子的各个位置进行标注。NER 任务中的常用模型包括生成式模型HMM、判别式模型CRF等。条件随机场（Conditional Random Field，CRF）是NER目前的主流模型。它的目标函数不仅考虑输入的状态特征函数，而且还包含了标签转移特征函数。在训练时可以使用SGD学习模型参数。在已知模型时，给输入序列求预测输出序列即求使目标函数最大化的最优序列，是一个动态规划问题，可以使用Viterbi算法解码来得到最优标签序列。CRF的优点在于其为一个位置进行标注的过程中可以利用丰富的内部及上下文特征信息。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/summary/summary_2.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;       近年来，随着硬件计算能力的发展以及词的分布式表示（word embedding）的提出，神经网络可以有效处理许多NLP任务。这类方法对于序列标注任务（如CWS、POS、NER）的处理方式是类似的：将token从离散one-hot表示映射到低维空间中成为稠密的embedding，随后将句子的embedding序列输入到RNN中，用神经网络自动提取特征，Softmax来预测每个token的标签。
这种方法使得模型的训练成为一个端到端的过程，而非传统的pipeline，不依赖于特征工程，是一种数据驱动的方法，但网络种类繁多、对参数设置依赖大，模型可解释性差。此外，这种方法的一个缺点是对每个token打标签的过程是独立的进行，不能直接利用上文已经预测的标签（只能靠隐含状态传递上文信息），进而导致预测出的标签序列可能是无效的，例如标签I-PER后面是不可能紧跟着B-PER的，但Softmax不会利用到这个信息。
       LongShort Term Memory网络一般叫做LSTM，是RNN的一种特殊类型，可以学习长距离依赖信息。LSTM 由Hochreiter &amp;amp;Schmidhuber (1997)提出，并在近期被Alex Graves进行了改良和推广。在很多问题上，LSTM 都取得了相当巨大的成功，并得到了广泛的使用。LSTM 通过巧妙的设计来解决长距离依赖问题。
所有 RNN 都具有一种重复神经网络单元的链式形式。在标准的RNN中，这个重复的单元只有一个非常简单的结构。LSTM通过三个门结构（输入门，遗忘门，输出门），选择性地遗忘部分历史信息，加入部分当前输入信息，最终整合到当前状态并产生输出状态。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/summary/summary_3.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;       应用于NER中的BiLSTM-CRF模型主要由Embedding层（主要有词向量，字向量以及一些额外特征），双向LSTM层，以及最后的CRF层构成。实验结果表明biLSTM-CRF已经达到或者超过了基于丰富特征的CRF模型，成为目前基于深度学习的NER方法中的最主流模型。在特征方面，该模型继承了深度学习方法的优势，无需特征工程，使用词向量以及字符向量就可以达到很好的效果，如果有高质量的词典特征，能够进一步获得提高。&lt;/p&gt;

&lt;h3 id=&quot;2词性标注&quot;&gt;2.词性标注&lt;/h3&gt;
&lt;p&gt;       词性标注（part-of-speech tagging）,又称为词类标注或者简称标注，是指为分词结果中的每个单词标注一个正确的词性的程序，也即确定每个词是名词、动词、形容词或者其他词性的过程。
词性标注这里基本可以照搬分词的工作，在汉语中，大多数词语只有一个词性，或者出现频次最高的词性远远高于第二位的词性。据说单纯选取最高频词性，就能实现80%准确率的中文词性标注程序。
主要可以分为基于规则和基于统计的方法，下面列举几种统计方法：
（1）基于最大熵的词性标注
（2）基于统计最大概率输出词性
词性标注（POS）具有许多应用，包括文本解析，文本语音转换，信息抽取等。在《Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Recurrent Neural Network》工作中，提出了一个采用RNN进行词性标注的系统。该模型采用《Wall Street Journal data from Penn Treebank III》数据集进行了测试，并获得了97.40％的标记准确性。&lt;/p&gt;

&lt;h3 id=&quot;3文本分类&quot;&gt;3.文本分类&lt;/h3&gt;
&lt;p&gt;       文本分类问题算是自然语言处理领域中一个非常经典的问题了，相关研究最早可以追溯到上世纪50年代，当时是通过专家规则（Pattern）进行分类，甚至在80年代初一度发展到利用知识工程建立专家系统，这样做的好处是短平快的解决top问题，但显然天花板非常低，不仅费时费力，覆盖的范围和准确率都非常有限。后来伴随着统计学习方法的发展，特别是90年代后互联网在线文本数量增长和机器学习学科的兴起，逐渐形成了一套解决大规模文本分类问题的经典玩法，这个阶段的主要套路是人工特征工程+浅层分类模型。整个文本分类问题就拆分成了特征工程和分类器两部分。
       传统做法主要问题的文本表示是高纬度高稀疏的，特征表达能力很弱，而且神经网络很不擅长对此类数据的处理；此外需要人工进行特征工程，成本很高。而深度学习最初在之所以图像和语音取得巨大成功，一个很重要的原因是图像和语音原始数据是连续和稠密的，有局部相关性。应用深度学习解决大规模文本分类问题最重要的是解决文本表示，再利用CNN/RNN等网络结构自动获取特征表达能力，去掉繁杂的人工特征工程，端到端的解决问题。下面是深度学习在文本分类的方法：分布式表示（Distributed Representation）其实Hinton 最早在1986年就提出了，基本思想是将每个词表达成 n 维稠密、连续的实数向量，与之相对的one-hot encoding向量空间只有一个维度是1，其余都是0。分布式表示最大的优点是具备非常powerful的特征表达能力，比如 n 维向量每维 k 个值，可以表征用深度学习（CNN RNN Attention）解决大规模文本分类问题 - 综述和实践 个概念。事实上，不管是神经网络的隐层，还是多个潜在变量的概率主题模型，都是应用分布式表示。下图是03年Bengio在《A Neural Probabilistic Language Model》中提出的网络结构：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/summary/summary_4.png&quot; style=&quot;zoom:50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;       这篇文章提出的神经网络语言模型（NNLM，Neural Probabilistic Language Model）采用的是文本分布式表示，即每个词表示为稠密的实数向量。NNLM模型的目标是构建语言模型。尽管Hinton 86年就提出了词的分布式表示，Bengio 03年便提出了NNLM，词向量真正火起来是google Mikolov 13年发表的两篇word2vec的文章 《Efficient Estimation of Word Representations in Vector Space》和《Distributed Representations of Words and Phrases and their Compositionality》，更重要的是发布了简单好用的word2vec工具包，在语义维度上得到了很好的验证，极大的推进了文本分析的进程。下图是文中提出的CBOW 和 Skip-Gram两个模型的结构，基本类似于NNLM，不同的是模型去掉了非线性隐层，预测目标不同，CBOW是上下文词预测当前词，Skip-Gram则相反。
       文本分类是许多应用程序中的重要组成部分，例如网络搜索，信息过滤，语言识别，可读性评估和情感分析。神经网络主要用于这些任务。
       Siwei Lai, Liheng Xu, Kang Liu, and Jun Zhao在论文《Recurrent Convolutional Neural Networks for Text Classification》中，提出了一种用于文本分类的循环卷积神经网络，该模型没有人为设计的特征。该团队在四个数据集测试了他们模型的效果，四个数据集包括：20Newsgroup（有四类，计算机，政治，娱乐和宗教），复旦大学集（中国的文档分类集合，包括20类，如艺术，教育和能源），ACL选集网（有五种语言：英文，日文，德文，中文和法文）和Sentiment Treebank数据集（包含非常负面，负面，中性，正面和非常正面的标签的数据集）。测试后，将模型与现有的文本分类方法进行比较，如Bag of Words，Bigrams + LR，SVM，LDA，Tree Kernels，RNN和CNN。最后发现，在所有四个数据集中，神经网络方法优于传统方法，他们所提出的模型效果优于CNN和循环神经网络。&lt;/p&gt;

&lt;h3 id=&quot;4语义分析和问题回答&quot;&gt;4.语义分析和问题回答&lt;/h3&gt;
&lt;p&gt;       下图是中文信息处理报告《第二章 语义分析研究进展、 现状及趋势》中的图。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/summary/summary_5.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;       任何对语言的理解都可以归为语义分析的范畴。一段文本通常由词、句子和段落来构成，根据理解对象的语言单位不同， 语义分析又可进一步分解为词汇级语义分析、句子级语义分析以及篇章级语义分析。语义分析的目标就是通过建立有效的模型和系统， 实现在各个语言单位 （包括词汇、句子和篇章等） 的自动语义分析，从而实现理解整个文本表达的真实语义。 
       对于不同的语言单位，语义分析的任务各不相同。在词的层次上，语义分析的基本任务是进行词义消歧（WSD），在句子层面上是语义角色标注（SRL），在篇章层面上是指代消歧，也称共指消解。
词义消歧
       由于词是能够独立运用的最小语言单位，句子中的每个词的含义及其在特定语境下的相互作用构成了整个句子的含义，因此，词义消歧是句子和篇章语义理解的基础，词义消歧有时也称为词义标注，其任务就是确定一个多义词在给定上下文语境中的具体含义。
       词义消歧的方法也分为有监督的消歧方法和无监督的消歧方法，在有监督的消歧方法中，训练数据是已知的，即每个词的词义是被标注了的；而在无监督的消歧方法中，训练数据是未经标注的。
多义词的词义识别问题实际上就是该词的上下文分类问题，还记得词性一致性识别的过程吗，同样也是根据词的上下文来判断词的词性。
       有监督词义消歧根据上下文和标注结果完成分类任务。而无监督词义消歧通常被称为聚类任务，使用聚类算法对同一个多义词的所有上下文进行等价类划分，在词义识别的时候，将该词的上下文与各个词义对应上下文的等价类进行比较，通过上下文对应的等价类来确定词的词义。此外，除了有监督和无监督的词义消歧，还有一种基于词典的消歧方法。
       在词义消歧方法研究中，我们需要大量测试数据，为了避免手工标注的困难，我们通过人工制造数据的方法来获得大规模训练数据和测试数据。其基本思路是将两个自然词汇合并，创建一个伪词来替代所有出现在语料中的原词汇。带有伪词的文本作为歧义原文本，最初的文本作为消歧后的文本。&lt;/p&gt;
&lt;h4 id=&quot;41有监督的词义消歧方法&quot;&gt;4.1有监督的词义消歧方法&lt;/h4&gt;
&lt;p&gt;有监督的词义消歧方法通过建立分类器，用划分多义词上下文类别的方法来区分多义词的词义。&lt;/p&gt;
&lt;h4 id=&quot;42基于互信息的消歧方法&quot;&gt;4.2基于互信息的消歧方法&lt;/h4&gt;
&lt;p&gt;       基于互信息的消歧方法基本思路是，对每个需要消歧的多义词寻找一个上下文特征，这个特征能够可靠地指示该多义词在特定上下文语境中使用的是哪种语义。
互信息是两个随机变量X和Y之间的相关性，X与Y关联越大，越相关，则互信息越大。
       这里简单介绍用在机器翻译中的Flip-Flop算法，这种算法适用于这样的条件，A语言中有一个词，它本身有两种意思，到B语言之后，有两种以上的翻译。我们现在有的，是B语言中该词的多种翻译，以及每种翻译所对应的上下文特征。我们需要得到的，是B语言中的哪些翻译对应义项1，哪些对应义项2。
       这个问题复杂的地方在于，对于普通的词义消歧，比如有两个义项的多义词，词都是同一个，上下文有很多，我们把这些上下文划分为两个等价类；而这种跨语言的，不仅要解决上下文的划分，在这之前还要解决两个义项多种词翻译的划分。这里面最麻烦的就是要先找到两种义项分别对应的词翻译，和这两种义项分别对应的词翻译所对应的上下文特征，以及他们之间的对应关系。
       想象一下，地上有两个圈，代表两个义项；这两个圈里，分别有若干个球，代表了每个义项对应的词翻译；然后这两个圈里还有若干个方块，代表了每个义项在该语言中对应的上下文。然后球和方块之间有线连着（球与球，方块与方块之间没有），随便连接，球可以连多个方块，方块也可以连多个球。然后，圈没了，两个圈里的球和方块都混在了一起，乱七八糟的，你该怎么把属于这两个圈的球和方块分开。
       Flip-Flop算法给出的方法是，试试。把方块分成两个集合，球也分成两个集合，然后看看情况怎么样，如果情况不好就继续试，找到最好的划分。然后需要解决的问题就是，怎么判定分的好不好？用互信息。如果两个上下文集（方块集）和两个词翻译集（球集）之间的互信息大，那我们就认为他们的之间相关关系大，也就与原来两个义项完美划分更接近。
       实际上，基于互信息的这种方法直接把词翻译的义项划分也做好了。&lt;/p&gt;
&lt;h4 id=&quot;43基于贝叶斯分类器的消歧方法&quot;&gt;4.3基于贝叶斯分类器的消歧方法&lt;/h4&gt;
&lt;p&gt;       基于贝叶斯分类器的消歧方法的思想与《浅谈机器学习基础》中讲的朴素贝叶斯分类算法相同，当时是用来判定垃圾邮件和正常邮件，这里则是用来判定不同义项（义项数可以大于2），我们只需要计算给定上下文语境下，概率最大的词义就好了。
       根据贝叶斯公式，两种情况下，分母都可以忽略，所要计算的就是分子，找最大的分子，在垃圾邮件识别中，分子是P(当前邮件所出现的词语|垃圾邮件)P(垃圾邮件)，那么乘起来就是垃圾邮件和当前邮件词语出现的联合分布概率，正常邮件同理；而在这里分子是P(当前词语所存在的上下文|某一义项)P(某一义项)，这样计算出来的就是某一义项和上下文的联合分布概率，再除以分母P(当前词语所存在的上下文)，计算出来的结果就是P(某一义项|当前词语所存在的上下文)，就能根据上下文去求得概率最大的义项了。&lt;/p&gt;
&lt;h4 id=&quot;44基于最大熵的词义消歧方法&quot;&gt;4.4基于最大熵的词义消歧方法&lt;/h4&gt;
&lt;p&gt;       利用最大熵模型进行词义消歧的基本思想也是把词义消歧看做一个分类问题，即对于某个多义词根据其特定的上下文条件（用特征表示）确定该词的义项。&lt;/p&gt;
&lt;h4 id=&quot;45基于词典的词义消歧方法&quot;&gt;4.5基于词典的词义消歧方法&lt;/h4&gt;
&lt;h4 id=&quot;46基于词典语义定义的消歧方法&quot;&gt;4.6基于词典语义定义的消歧方法&lt;/h4&gt;
&lt;p&gt;       M. Lesk 认为词典中的词条本身的定义就可以作为判断其词义的一个很好的条件，就比如英文中的core，在词典中有两个定义，一个是『松树的球果』，另一个是指『用于盛放其它东西的锥形物，比如盛放冰激凌的锥形薄饼』。如果在文本中，出现了『树』、或者出现了『冰』，那么这个core的词义就可以确定了。
       我们可以计算词典中不同义项的定义和词语在文本中上下文的相似度，就可以选择最相关的词义了。&lt;/p&gt;
&lt;h4 id=&quot;47基于义类词典的消歧方法&quot;&gt;4.7基于义类词典的消歧方法&lt;/h4&gt;
&lt;p&gt;       和前面基于词典语义的消歧方法相似，只是采用的不是词典里义项的定义文本，而是采用的整个义项所属的义类，比如ANMINAL、MACHINERY等，不同的上下文语义类有不同的共现词，依靠这个来对多义词的义项进行消歧。&lt;/p&gt;
&lt;h4 id=&quot;48无监督的词义消歧方法&quot;&gt;4.8无监督的词义消歧方法&lt;/h4&gt;
&lt;p&gt;       严格地讲，利用完全无监督的消歧方法进行词义标注是不可能的，因为词义标注毕竟需要提供一些关于语义特征的描述信息，但是，词义辨识可以利用完全无监督的机器学习方法实现。
       其关键思想在于上下文聚类，计算多义词所出现的语境向量的相似性就可以实现上下文聚类，从而实现词义区分。
       语义角色标注是一种浅层语义分析技术，它以句子为单位，不对句子所包含的予以信息进行深入分析，而只是分析句子的谓词-论元结构。具体一点讲，语义角色标注的任务就是以句子的谓词为中心，研究句子中各成分与谓词之间的关系，并且用语义角色来描述它们之间的关系。目前语义角色标注方法过于依赖句法分析的结果，而且领域适应性也太差。
       自动语义角色标注是在句法分析的基础上进行的，而句法分析包括短语结构分析、浅层句法分析和依存关系分析，因此，语义角色标注方法也分为基于短语结构树的语义角色标注方法、基于浅层句法分析结果的语义角色标注方法和基于依存句法分析结果的语义角色标注方法三种。
       它们的基本流程类似，在研究中一般都假定谓词是给定的，所要做的就是找出给定谓词的各个论元，也就是说任务是确定的，找出这个任务所需的各个槽位的值。其流程一般都由4个阶段组成：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/summary/summary_6.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;       候选论元剪除的目的就是要从大量的候选项中剪除掉那些不可能成为论元的项，从而减少候选项的数目。
       论元辨识阶段的任务是从剪除后的候选项中识别出哪些是真正的论元。论元识别通常被作为一个二值分类问题来解决，即判断一个候选项是否是真正的论元。该阶段不需要对论元的语义角色进行标注。
       论元标注阶段要为前一阶段识别出来的论元标注语义角色。论元标注通常被作为一个多值分类问题来解决，其类别集合就是所有的语义角色标签。
       最终，后处理阶段的作用是对前面得到的语义角色标注结果进行处理，包括删除语义角色重复的论元等。
       问题回答系统可以自动回答通过自然语言描述的不同类型的问题，包括定义问题，传记问题，多语言问题等。神经网络可以用于开发高性能的问答系统。
在《Semantic Parsing via Staged Query Graph Generation Question Answering with Knowledge Base》文章中，Wen-tau Yih等人描述了基于知识库来开发问答语义解析系统的框架框架。作者说他们的方法早期使用知识库来修剪搜索空间，从而简化了语义匹配问题。他们还应用高级实体链接系统和一个用于匹配问题和预测序列的深卷积神经网络模型。该模型在WebQuestions数据集上进行了测试，其性能优于以前的方法。&lt;/p&gt;

&lt;h3 id=&quot;5语言生成和多文档总结&quot;&gt;5.语言生成和多文档总结&lt;/h3&gt;
&lt;p&gt;       自然语言生成有许多应用，如自动撰写报告，基于零售销售数据分析生成文本，总结电子病历，从天气数据生成文字天气预报，甚至生成笑话。
       研究人员在最近的一篇论文《Natural Language Generation, Paraphrasing and Summarization of User Reviews with Recurrent Neural Networks》中，描述了基于循环神经网络（RNN）模型，能够生成新句子和文档摘要的。该论文描述和评估了俄罗斯语820,000个消费者的评论数据库。网络的设计允许用户控制生成的句子的含义。通过选择句子级特征向量，可以指示网络学习，例如，“在大约十个字中说出一个关于屏幕和音质的东西”。语言生成的能力可以生成具有不错质量的，多个用户评论的抽象摘要。通常，总结报告使用户可以快速获取大型文档集中的主要信息。&lt;/p&gt;

&lt;h3 id=&quot;6机器翻译&quot;&gt;6.机器翻译&lt;/h3&gt;
&lt;p&gt;       机器翻译软件在世界各地使用，尽管有限制。在某些领域，翻译质量不好。为了改进结果，研究人员尝试不同的技术和模型，包括神经网络方法。《Neural-based Machine Translation for Medical Text Domain》研究的目的是检查不同训练方法对用于，采用医学数据的，波兰语-英语机器翻译系统的影响。采用The European Medicines Agency parallel text corpus来训练基于神经网络和统计机器翻译系统。证明了神经网络需要较少的训练和维护资源。另外，神经网络通常用相似语境中出现的单词来替代其他单词。&lt;/p&gt;

&lt;h3 id=&quot;7语音识别&quot;&gt;7.语音识别&lt;/h3&gt;
&lt;p&gt;       语音识别应用于诸如家庭自动化，移动电话，虚拟辅助，免提计算，视频游戏等诸多领域。神经网络在这一领域得到广泛应用。
       在《Convolutional Neural Networks for Speech Recognition》文章中，科学家以新颖的方式解释了如何将CNN应用于语音识别，使CNN的结构直接适应了一些类型的语音变化，如变化的语速。在TIMIT手机识别和大词汇语音搜索任务中使用。&lt;/p&gt;

&lt;h3 id=&quot;8字符识别&quot;&gt;8.字符识别&lt;/h3&gt;
&lt;p&gt;       字符识别系统具有许多应用，如收据字符识别，发票字符识别，检查字符识别，合法开票凭证字符识别等。文章《Character Recognition Using Neural Network》提出了一种具有85％精度的手写字符的方法。
       以上是通过查阅相关文献得到的最近行业进展及相关技术，深度学习通过在某些任务中极佳的表现正在改革机器学习。由于模仿了人类大脑，深度学习可以运用在很多领域中。深度学习方法在会话识别、图像识别、对象侦测以及如药物发现和基因组学等领域表现出了惊人的准确性。自然语言处理研究逐渐从词汇语义成分的语义转移，进一步的，叙事的理解。然而人类水平的自然语言处理，是一个人工智能完全问题。它是相当于解决中央的人工智能问题使计算机和人一样聪明，或强大的AI。学好自然语言处理任重而道远，因此个人搭建了博客w https://provenclei.github.io 提供交流和访问，有时也会在Git上传自己的代码。&lt;/p&gt;

&lt;h2 id=&quot;参考文献&quot;&gt;参考文献&lt;/h2&gt;

&lt;p&gt;《Convolutional Neural Networks for Sentence Classification》    Yoon Kim&lt;/p&gt;

&lt;p&gt;《Neural-based Machine Translation for Medical Text Domain》     Krzysztof Wołk，Krzysztof Marasek&lt;/p&gt;

&lt;p&gt;《Recurrent Convolutional Neural Network for Text Classification》      Seiwei Lai, LiHeng Xu, Kang Liu, Jun Zhao&lt;/p&gt;

&lt;p&gt;《Semantic Parsing via Staged Query Graph Generation Question Answering with Knowledge Base》     Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and Jianfeng Gao&lt;/p&gt;

&lt;p&gt;《Natural Language Generation, Paraphrasing and Summarization of User Reviews with Recurrent Neural Networks》     Rafael Ferreira，George D.C. Cavalcanti，Fred Freitas;&lt;/p&gt;

&lt;p&gt;《Recent advances in conversational speech recognition using convolutional and recurrent neural networks》     G. Saon，M. Picheny&lt;/p&gt;

&lt;p&gt;《Convolutional Neural Networks for Speech Recognition》Chen, Mingyi，He, Xuanji，Yang, Jing，Zhang&lt;/p&gt;

&lt;p&gt;《Character Recognition Using Neural Network》    Jakhro Abdul Naveed&lt;/p&gt;

&lt;p&gt;《CNN支持下的领域文本自组织映射神经网络聚类算法》     贾声声，彭敦陆&lt;/p&gt;

&lt;p&gt;《激活函数导向的RNN算法优化》    张尧&lt;/p&gt;

&lt;p&gt;《基于双向RNN的信息提取系统》    刘世林，何宏靖&lt;/p&gt;

&lt;p&gt;《一种基于自然语言处理的环境科学命名 实体识别方法》    张永富，李志宏，李军军，程树东&lt;/p&gt;

&lt;p&gt;《基于自然语言处理的文本情感分析方法与系》     晋彤，张中弦&lt;/p&gt;

&lt;p&gt;《一种基于自然语言句法分析树的机器学习情感分析器》      唐新怀，蒋戈，胡月，胡晓博，施维&lt;/p&gt;

&lt;p&gt;《基于双重注意力模型的微博情感分析方法》     张仰森，郑佳，黄改娟，蒋玉茹&lt;/p&gt;

&lt;p&gt;《用于微博情感分析的一种情感语义增强的深度学习模型》    何炎祥，孙松涛，牛菲菲，李飞&lt;/p&gt;</content><author><name>被水淹死的鱼</name></author><category term="自然语言处理" /><category term="综述" /><summary type="html">深度学习在自然语言处理的发展与应用</summary></entry><entry><title type="html">Non-parameter model: Naive Bayes</title><link href="http://localhost:4000/2018/10/15/Naive-Bayes.html" rel="alternate" type="text/html" title="Non-parameter model: Naive Bayes" /><published>2018-10-15T00:00:00+08:00</published><updated>2018-10-15T00:00:00+08:00</updated><id>http://localhost:4000/2018/10/15/Naive%20Bayes</id><content type="html" xml:base="http://localhost:4000/2018/10/15/Naive-Bayes.html">&lt;h1 id=&quot;naive-bayes-algorithm&quot;&gt;Naive Bayes algorithm&lt;/h1&gt;

&lt;h2 id=&quot;一-贝叶斯网络和贝叶斯分类器&quot;&gt;一. 贝叶斯网络和贝叶斯分类器&lt;/h2&gt;

&lt;h3 id=&quot;11-贝叶斯概率&quot;&gt;1.1 贝叶斯概率&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/bayes/bayes_7.png&quot; style=&quot;zoom:50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;       下面通过一个简单的例子介绍贝叶斯公式：如上图所示，假设我们有两个盒子，一个红色的，一个蓝色的。红色盒子中有&lt;code class=&quot;highlighter-rouge&quot;&gt;2&lt;/code&gt;个苹果和&lt;code class=&quot;highlighter-rouge&quot;&gt;6&lt;/code&gt;个橘子，蓝色盒子中有&lt;code class=&quot;highlighter-rouge&quot;&gt;3&lt;/code&gt;个苹果和&lt;code class=&quot;highlighter-rouge&quot;&gt;1&lt;/code&gt;个橘子。现假定随机选择一个盒子，再从盒子中随机选择一个水果，观察水果种类后放回。假设多次重复这个过程。在例子中我们定义选择盒子为一个随机事件，随机变量记为&lt;code class=&quot;highlighter-rouge&quot;&gt;B&lt;/code&gt;，&lt;code class=&quot;highlighter-rouge&quot;&gt;B&lt;/code&gt;的取值有两种，&lt;code class=&quot;highlighter-rouge&quot;&gt;r&lt;/code&gt;（对应红盒子）或&lt;code class=&quot;highlighter-rouge&quot;&gt;b&lt;/code&gt;（对应蓝盒子）。同理，在盒子中取水果也为定义为一个随机变量&lt;code class=&quot;highlighter-rouge&quot;&gt;F&lt;/code&gt;，F取值&lt;code class=&quot;highlighter-rouge&quot;&gt;a&lt;/code&gt;（苹果）或&lt;code class=&quot;highlighter-rouge&quot;&gt;o&lt;/code&gt;（橘子）。由此，我们得出以下两个公式：
\[ P(B = r) = \frac{4}{10} \]
\[P (B = b) = \frac{6}{10} \]  &lt;br /&gt;
       由以上两个公式我们可以得到结论：抽中蓝盒子的概率比抽中红盒子的概率大。如果在我们知道水果种类之前，有人问那个盒子更可能被选中，那么得到的最多的信息就是&lt;code class=&quot;highlighter-rouge&quot;&gt;P(B)&lt;/code&gt;，我们称之为&lt;code class=&quot;highlighter-rouge&quot;&gt;先验概率&lt;/code&gt;。因为他是我们在观察水果种类之前能够得到的概率。  &lt;br /&gt;
       下面我们对水果种类进行研究，可以得到以下公式
\[ P(F = a | B = r) = \frac{1}{4} \]
\[ P(F = o | B = r) = \frac{3}{4} \]
\[ P(F = a | B = b) = \frac{3}{4} \]
\[ P(F = o | B = b) = \frac{1}{4} \]
注意这些公式是归一化的，所以
\[ P(F = a | B = r) + P(F = o | B = r) = 1  \]
\[ P(F = a | B = b) + P(F = o | B = b) = 1 \] 
由概率论的基本规则，可以得出选择一个苹果的整体概率：
\[ P(F = a) = P(F = a | B = r)P(B = r) + P(F = o | B = b)P(B = b) = \frac{11}{20} \]
同理可得
\[ P(F = a) = \frac{9}{20} \] 
一旦我们知道拿出的是橘子，那么我们能够使用贝叶斯定理来计算在那个盒子中去的的概率&lt;code class=&quot;highlighter-rouge&quot;&gt;P(B|F)&lt;/code&gt;。
\[ P(B = r|F=o) = \frac{P(F = o|B = r)P(B = r)}{P(F = o)} = \frac{2}{3} \]
这样的概率我们称之为&lt;code class=&quot;highlighter-rouge&quot;&gt;后验概率&lt;/code&gt;。  &lt;br /&gt;
       由以上内容可以观察到，贝叶斯公式是将先验概率转化为后验概率。贝叶斯公式应用在学习数据时有一下表现。训练数据：
\[ \cal{D} = \lbrace{t_1, t_2, … , t_n}\rbrace \]
参数
\[ \omega = \lbrace \omega_1, \omega_2, …, \omega_n\rbrace \]
在观察到数据之前，我们会对参数&lt;code class=&quot;highlighter-rouge&quot;&gt;ω&lt;/code&gt;作出假设，这由先验公式&lt;code class=&quot;highlighter-rouge&quot;&gt;P=(ω)&lt;/code&gt;的形式给出。观察数据&lt;code class=&quot;highlighter-rouge&quot;&gt;D&lt;/code&gt;的效果可以通过条件概率&lt;code class=&quot;highlighter-rouge&quot;&gt;P(D|ω)&lt;/code&gt;表达。贝叶斯定理的形式为：
\[ P(\omega | \cal{D}) = \frac{ P(\cal{D} | \omega)P(\omega)}{P(\cal{D})} \] 
让我们能够观察后验概率&lt;code class=&quot;highlighter-rouge&quot;&gt;P(ω|D)&lt;/code&gt;，在观测到&lt;code class=&quot;highlighter-rouge&quot;&gt;D&lt;/code&gt;后估计&lt;code class=&quot;highlighter-rouge&quot;&gt;ω&lt;/code&gt;的不确定性。&lt;code class=&quot;highlighter-rouge&quot;&gt;P(D|ω)&lt;/code&gt;称为&lt;code class=&quot;highlighter-rouge&quot;&gt;似然函数(likelihood function)&lt;/code&gt;，它表达了在不同参数向量&lt;code class=&quot;highlighter-rouge&quot;&gt;ω&lt;/code&gt;下，观测数据出现的可能性大小。上述公式的分母是一个归一化参数，确保左侧是一个合理的概率密度，积分为&lt;code class=&quot;highlighter-rouge&quot;&gt;1&lt;/code&gt;。我们可以用后验概率分布和似然函数来表达贝叶斯定理的分母
\[ P(\cal{D}) = \int{P(\cal{D}|\omega)·P(\omega)}\,{\rm d}\omega \]
我们通过自言语言表达贝叶斯定理：
\[ posterior ∝ likelihood × prior \]&lt;/p&gt;

&lt;h3 id=&quot;12-高斯分布&quot;&gt;1.2 高斯分布&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/bayes/bayes_8.png&quot; alt=&quot;1&quot; /&gt;&lt;br /&gt;
D维向量x的高斯分布定义如下：
&lt;img src=&quot;/assets/bayes/bayes_9.png&quot; alt=&quot;1&quot; /&gt;&lt;br /&gt;
其中D维向量μ被称为均值，D × D的矩阵Σ被称为协方差，|Σ|表示Σ的行列式。
我们假定各次观 测是独立地从高斯分布中抽取的，分布的均值&lt;code class=&quot;highlighter-rouge&quot;&gt;μ&lt;/code&gt;和方差&lt;code class=&quot;highlighter-rouge&quot;&gt;σ2&lt;/code&gt;未知，我们想根据数据集来确定这些参数。独立地从相同的数据点中抽取的数据点被称为独立同分布(independent and identically distributed)，通常缩写成i.i.d.。我们已经看到两个独立事件的联合概率可以由各个事件的边缘概率的乘积得到。由于我们的数据集是独立同分布的，因此给定&lt;code class=&quot;highlighter-rouge&quot;&gt;μ&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;σ2&lt;/code&gt;，我们可以给出数据集的概率:
\[ p(x | μ,σ_2) = \prod_{n=1}^N{\cal{N}(x_n | μ,σ_2)} \]
其对数似然函数可以写成：
&lt;img src=&quot;/assets/bayes/bayes_9.png&quot; alt=&quot;1&quot; /&gt;&lt;br /&gt;
现在让我们思考线性拟合问题，曲线拟合问题的目标是能够根据N个输入&lt;code class=&quot;highlighter-rouge&quot;&gt;x = (x1,...,xN)T&lt;/code&gt;组成的数据集和它们对应的目标值&lt;code class=&quot;highlighter-rouge&quot;&gt;t = (t1, . . . , tN )T&lt;/code&gt; ，在给出输入变量&lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt;的新值的情况下，对目标变量&lt;code class=&quot;highlighter-rouge&quot;&gt;t&lt;/code&gt;进行预测。我们所构造的对数似然函数为:
\[ P({\bf {t}}|{\bf{x,\omega}},\beta) = -\frac{\beta}{2}\sum_{n=1}^N{[y(x_n,\omega) - t_n]^2} + \frac{N}{2}ln{\beta} - \frac{N}{2}ln{2\pi} \]
这些由公式关于&lt;code class=&quot;highlighter-rouge&quot;&gt;ω&lt;/code&gt;来确定。为了达到这个目的，我们可以省略公式右侧的最后两项，因为他们不依赖于&lt;code class=&quot;highlighter-rouge&quot;&gt;ω&lt;/code&gt;。并且，我们注意到，使用一个正的常数系数来缩放对数似然函数并不会改变关于&lt;code class=&quot;highlighter-rouge&quot;&gt;ω&lt;/code&gt;的最大值的位置， 因此我们可以用 &lt;code class=&quot;highlighter-rouge&quot;&gt;1&lt;/code&gt;来代替系数 &lt;code class=&quot;highlighter-rouge&quot;&gt;β&lt;/code&gt; 。最后，我们不去最大化似然函数，而是等价地去最小化负对数似然函数。于是我们看到，目前为止对于确定&lt;code class=&quot;highlighter-rouge&quot;&gt;ω&lt;/code&gt;的问题来说，最大化似然函数等价于最小化由
\[ E(\omega) = \frac{\beta}{2}\sum_{n=1}^N{[y - t_n]^2} \]
定义的平方和误差函数。因此，在高斯噪声的假设下，&lt;strong&gt;平方和误差函数是最大化似 然函数的一个自然结果&lt;/strong&gt;。&lt;/p&gt;

&lt;h3 id=&quot;13-贝叶斯网络&quot;&gt;1.3 贝叶斯网络&lt;/h3&gt;

&lt;p&gt;       贝叶斯网络是一个带有概率注释的有向无环图，图中的每一个结点均表示一个随机变量，图中两结点间若存在着一条弧，则表示这两结点相对应的随机变量是概率相依的，反之则说明这两个随机变量是条件独立的。网络中任意一个结点 &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt; 均有一个相应的&lt;code class=&quot;highlighter-rouge&quot;&gt;条件概率表 (Conditional Probability Table，CPT)&lt;/code&gt;，用以表示结点&lt;code class=&quot;highlighter-rouge&quot;&gt; X&lt;/code&gt; 在其父结点取各可能值时的条件概率。若结点 &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt; 无父结点,则 &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt; 的 &lt;code class=&quot;highlighter-rouge&quot;&gt;CPT&lt;/code&gt; 为其先验概率分布。贝叶斯网络的结构及各结点的 &lt;code class=&quot;highlighter-rouge&quot;&gt;CPT&lt;/code&gt; 定义了网络中各变量的概率分布。 
       贝叶斯分类器是用于分类的贝叶斯网络。该网络中应包含类结点 &lt;code class=&quot;highlighter-rouge&quot;&gt;C&lt;/code&gt;，其中 &lt;code class=&quot;highlighter-rouge&quot;&gt;C&lt;/code&gt; 的取值来自于类集合
\[( c_1 , c_2 , … , c_m),\]
还包含一组结点 
\[X = ( X_1 , X_2 , … , X_n)，\]
表示用于分类的特征。 对于贝叶斯网络分类器，若某一待分类的样本 &lt;code class=&quot;highlighter-rouge&quot;&gt;D&lt;/code&gt;，其分类特征值为 
\[x = ( x_1 , x_2 , … , x_n)，\] 
则样本 &lt;code class=&quot;highlighter-rouge&quot;&gt;D&lt;/code&gt; 属于类别 &lt;code class=&quot;highlighter-rouge&quot;&gt;ci&lt;/code&gt; 的概率
\[P( C = c_i　|　 X_1 = x_1 , X_2 = x_2 , … , X_n = x_n)，( i = 1 ,2 , … , m)\] 
应满足下式: &lt;br /&gt;
\[ P( C = c_i|X = x) = Max\lbrace P(C = c_1|X = x)P(C = c_2|X = x),…,P(C = c_m|X = x)\rbrace \]
而由贝叶斯公式:
\[P( C = c_i　|　X = x) = \frac{P( X = x　|　C = c_i) * P( C = c_i)}{P( X = x)}\]
其中，&lt;code class=&quot;highlighter-rouge&quot;&gt;P(C=ci)&lt;/code&gt; 可由领域专家的经验得到,而 &lt;code class=&quot;highlighter-rouge&quot;&gt;P(X=x|C=ci)&lt;/code&gt; 和 &lt;code class=&quot;highlighter-rouge&quot;&gt;P(X=x)&lt;/code&gt; 的计算则较困难。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;应用贝叶斯网络分类器进行分类主要分成两阶段：
第一阶段是贝叶斯网络分类器的学习，即从样本数据中构造分类器，包括结构学习和 CPT 学习。
第二阶段是贝叶斯网络分类器的推理，即计算类结点的条件概率，对分类数据进行分类。  &lt;br /&gt;
这两个阶段的时间复杂性均取决于特征值间的依赖程度，甚至可以是 NP 完全问题，因而在实际应用中，往往需要对贝叶斯网络分类器进行简化。根据对特征值间不同关联程度的假设，可以得出各种贝叶斯分类器，Naive Bayes、TAN、BAN、GBN 就是其中较典型、研究较深入的贝叶斯分类器。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;二-常见的贝叶斯分类器&quot;&gt;二. 常见的贝叶斯分类器&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/bayes/bayes_1.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;       贝叶斯分类器的分类原理是通过某对象的先验概率，利用贝叶斯公式计算出其后验概率，即该对象属于某一类的概率，选择具有最大后验概率的类作为该对象所属的类。目前研究较多的贝叶斯分类器主要有四种，分别是:Naive Bayes、TAN、BAN 和 GBN。&lt;/p&gt;

&lt;h3 id=&quot;21-naive-bayes-分类器&quot;&gt;2.1 Naive Bayes 分类器&lt;/h3&gt;
&lt;blockquote&gt;
  &lt;p&gt;为了简化计算，最简单的情形可以假定各变量 x 是相对独立的，即为 NB(Naive-Bayes)分类器，如图一所示，虽然这种条件独立的假设在许多应用梁宇未必能很好地满足，但这种简化的贝叶斯分类器在许多实际应用中还是得到了较好的分类精度。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;22-tan-分类器&quot;&gt;2.2 TAN 分类器&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;TAN（Tree Augmented Naive-Bayes）分类器对 NB 分类器进行扩展，允许各特征变量所对应的节点都成一棵树，如图二。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;朴素贝叶斯(NB)的‘朴素’体现在它假设各属性之间没有相互依赖，可以简化贝叶斯公式中`P(x|c)`的计算。但事实上，属性直接完全没有依赖的情况是非常少的。如果简单粗暴用朴素贝叶斯建模，泛化能力和鲁棒性都难以达到令人满意。这就可以引出半朴素贝叶斯了，它假定每个属性最多只依赖一个(或k个)其他属性。它考虑属性间的相互依赖，但假定依赖只有一个(ODE)或k个(kDE)其他属性。这就是半朴素贝叶斯的’半’所体现之处。 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/bayes/bayes_1.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;常见的半朴素贝叶斯算法SPODE和TAN
    SPODE算法：假设所有属性都依赖同一个属性，这个属性称为“超父”属性。 
    TAN算法：通过最大带权生成树算法确定属性之间的依赖关系，简单点说，就是每个属性找到跟自己最相关的属性，然后形成一个有向边（只能往一个方向）。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;23-ban-分类器&quot;&gt;2.3 BAN 分类器&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;BAN（BN Augmented Naive-Baye）进一步扩展 TAN 分类器，允许各特征变量所对应的节点构成一个图，如图三。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;24-gbn-分类器&quot;&gt;2.4 GBN 分类器&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;GBN（General Bayesian Network）是一种无约束的贝叶斯网络分类器，和前三种贝叶斯网络分类器有较大区别的是，在前三类分类器中均将类变量所对应的节点作为特殊的节点，即是各特征接待你的父节点，而 GBN 中将类节点作为一不同节点，如图四所示。 &lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;三-bayes-估计&quot;&gt;三. Bayes 估计&lt;/h2&gt;
&lt;h3 id=&quot;31-贝叶斯估计&quot;&gt;3.1 贝叶斯估计&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/bayes/bayes_3.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;       朴素贝叶斯分类是将一个未知样本分到几个预先已知类的过程。过程中最重要的就是建立模型，描述预先的数据集或概念集。通过分析由属性描述的样本(或实例，对象等)来构造模型。假定每一个样本都有一个预先定义的类，由一个被称为类标签的属性确定。为建立模型而被分析的数据元组形成训练数据集，该步也称作有指导的学习。   &lt;br /&gt;
       在众多的分类模型中，应用最为广泛的两种分类模型是&lt;code class=&quot;highlighter-rouge&quot;&gt;决策树模型(Decision Tree Model)&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;朴素贝叶斯模型(Naive Bayesian Model，NBM)&lt;/code&gt;。决策树模型通过构造树来解决分类问题。首先利用训练数据集来构造一棵决策树，一旦树建立起来，它就可为未知样本产生一个分类。 在分类问题中使用决策树模型有很多的优点，决策树便于使用，而且高效；根据决策树可以 很容易地构造出规则，而规则通常易于解释和理解；决策树可很好地扩展到大型数据库中，同时它的大小独立于数据库的大小；决策树模型的另外一大优点就是可以对有许多属性的数 据集构造决策树。决策树模型也有一些缺点，比如处理缺失数据时的困难，过度拟合问题的出现，以及忽略数据集中属性之间的相关性等。  &lt;br /&gt;
       和决策树模型相比，朴素贝叶斯模型发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率。同时，&lt;code class=&quot;highlighter-rouge&quot;&gt;NBM&lt;/code&gt; 模型所需估计的参数很少，对缺失数据不太敏感，算法也比较简单。理论上，&lt;code class=&quot;highlighter-rouge&quot;&gt;NBM&lt;/code&gt; 模型与其他分类方法相比具有最小的误差率。但是实际上并非总是如此， 这是因为 &lt;code class=&quot;highlighter-rouge&quot;&gt;NBM&lt;/code&gt; 模型假设属性之间相互独立，这个假设在实际应用中往往是不成立的，这给 &lt;code class=&quot;highlighter-rouge&quot;&gt;NBM&lt;/code&gt; 模型的正确分类带来了一定影响。在属性个数比较多或者属性之间相关性较大时，&lt;code class=&quot;highlighter-rouge&quot;&gt;NBM&lt;/code&gt; 模型的分类效率比不上决策树模型。而在属性相关性较小时，&lt;code class=&quot;highlighter-rouge&quot;&gt;NBM&lt;/code&gt; 模型的性能最为良好。&lt;/p&gt;

&lt;h3 id=&quot;32-拉普拉斯平滑laplace-smoothing&quot;&gt;3.2 拉普拉斯平滑（Laplace smoothing）&lt;/h3&gt;
&lt;p&gt;       &lt;code class=&quot;highlighter-rouge&quot;&gt;拉普拉斯平滑&lt;/code&gt;在自然语言处理中非常常见。用极大似然估计可能出现所要估计的概率值为&lt;code class=&quot;highlighter-rouge&quot;&gt;0&lt;/code&gt;的情况，这时会影响到后验概率的计算结果，使分类产生偏差。也就是当某个分量在总样本某个分类中（观察样本库/训练集）从没出现过，会导致整个实例的计算结果为&lt;code class=&quot;highlighter-rouge&quot;&gt;0&lt;/code&gt;。为了解决这个问题，使用拉普拉斯平滑进行处理。   &lt;br /&gt;
       它的思想非常简单，就是对先验概率的分子（划分的计数）加1，分母加上类别数；对条件概率分子加1，分母加上对应特征的可能取值数量。这样在解决零概率问题的同时，也保证了概率和依然为1。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;举个栗子：
假设在文本分类中，有3个类，A、B、C，在指定的训练样本中，某段话F，在各个类中观测计数分别为0，990，10，即概率为P(F/A)=0，P(F/B)=0.99，P(F/C)=0.01，对这三个量使用拉普拉斯平滑的计算方法如下：P(F/A)= 1/1003 = 0.001，P(F/B)= 991/1003=0.988 P(F/C)= 11/1003=0.011&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;四-naive-bayes-在分类中的应用&quot;&gt;四. Naive Bayes 在分类中的应用&lt;/h2&gt;
&lt;p&gt;以下是 Tom M.Mitchell 的机器学习中的一个栗子，是对不同环境下是否适合打网球作出预测，很具有代表性。&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Play-Tennis Problem&lt;/p&gt;
  &lt;ol&gt;
    &lt;li&gt;Traing Data
 &lt;img src=&quot;/assets/bayes/bayes_4.png&quot; alt=&quot;1&quot; /&gt;&lt;/li&gt;
    &lt;li&gt;Learning
 &lt;img src=&quot;/assets/bayes/bayes_5.png&quot; alt=&quot;1&quot; /&gt;&lt;/li&gt;
    &lt;li&gt;Prediction
 &lt;img src=&quot;/assets/bayes/bayes_6.png&quot; alt=&quot;1&quot; /&gt;&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;五-naive-bayes-在拟合中的应用&quot;&gt;五. Naive Bayes 在拟合中的应用&lt;/h2&gt;
&lt;p&gt;       虽然我们已经谈到了先验分布&lt;code class=&quot;highlighter-rouge&quot;&gt;p(ω|α)&lt;/code&gt;，但是我们目前仍然在进行&lt;code class=&quot;highlighter-rouge&quot;&gt;ω&lt;/code&gt;的点估计，这并不是贝叶斯观点。在一个纯粹的贝叶斯方法中，我们应该自始至终地应用概率的加和规则和乘积规则。我们稍后会看到，这需要对所有&lt;code class=&quot;highlighter-rouge&quot;&gt;ω&lt;/code&gt;值进行积分。对于模式识别来说，这种积分是贝叶斯方法的核心。 
       在曲线拟合问题中，我们知道训练数据&lt;strong&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt;&lt;/strong&gt;和&lt;strong&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;t&lt;/code&gt;&lt;/strong&gt;，以及一个新的测试点&lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt;，我们的目标是预测&lt;code class=&quot;highlighter-rouge&quot;&gt;t&lt;/code&gt;的值。我们要假设参数α和β是固定已知的。 我们想估计预测分布
\[ P(t|x, {\bf{x, t}}) = \int{P(t|x, \omega)P({\bf\omega|{x, t}})} \,{\rm d}\omega \]
简单地说，贝叶斯方法就是自始至终地使用概率的加和规则和乘积规则。
&lt;img src=&quot;/assets/bayes/bayes_11.png&quot; alt=&quot;1&quot; /&gt; 
&lt;img src=&quot;/assets/bayes/bayes_12.png&quot; style=&quot;zoom:50%&quot; /&gt;
用贝叶斯方法处理多项式曲线拟合问题得到的预测分布的结果。使用的多项式为&lt;code class=&quot;highlighter-rouge&quot;&gt;M = 9&lt;/code&gt;，超参数被固定为&lt;code class=&quot;highlighter-rouge&quot;&gt;α = 5 × 10−3&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;β = 11.1&lt;/code&gt;(对应于已知的噪声方差)。其中，红色曲线表示预测概率分布的均值，红色区域对应于均值周围&lt;code class=&quot;highlighter-rouge&quot;&gt;±1&lt;/code&gt;标准差的范围。这个例子来自于《Pattern Recognition and Machine Learning》的贝叶斯曲线拟合。&lt;/p&gt;

&lt;h2 id=&quot;六-参考文献&quot;&gt;六. 参考文献&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;《四种贝叶斯分类器及其比较》邓甦，付长贺    

《Pattern Recognition and Machine Learning》 Christopher M.Bishop    

《统计学习方法》李航

《Machine Learning》 Tom M.Mitchell
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>被水淹死的鱼</name></author><category term="机器学习" /><summary type="html">Naive Bayes algorithm</summary></entry><entry><title type="html">Non-parameter model: KNN</title><link href="http://localhost:4000/2018/10/08/KNN.html" rel="alternate" type="text/html" title="Non-parameter model: KNN" /><published>2018-10-08T00:00:00+08:00</published><updated>2018-10-08T00:00:00+08:00</updated><id>http://localhost:4000/2018/10/08/KNN</id><content type="html" xml:base="http://localhost:4000/2018/10/08/KNN.html">&lt;h1 id=&quot;k-nearest-neighber-algorithm&quot;&gt;K-Nearest Neighber algorithm&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;KNN is a non-parameteric model used for classification and regression. The input considts of the K closest training examples in the feature space.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;一-knn-算法决策过程&quot;&gt;一. KNN 算法决策过程&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/knn/knn_1.png&quot; style=&quot;zoom:50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;四角星要被决定赋予哪个类，是白色圆圈还是黑色圆圈?  &lt;br /&gt;
如果 K=3，由于黑色圆圈所占比例为 2/3，四角星将被赋予黑色圆圈那个类。  &lt;br /&gt;
如果 K=7，由于黑色圈圈比例为 5/7，四角星被赋予黑色与圆圈类。&lt;/p&gt;

&lt;p&gt;K 最近邻(k-Nearest Neighbor，KNN)分类算法，是一个理论上比较成熟的方法，也是最简单的机器学习算法之一。该方法的&lt;strong&gt;思路&lt;/strong&gt;是：如果一个样本在特征空间中的 k 个最相似（即特征空间中最邻近）的样本中的大多数属于某一个类别，则该样本也属于这个类别。KNN 算法中，所选择的邻居都是&lt;strong&gt;已经正确分类的对象&lt;/strong&gt;。该方法在定类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。KNN 方法虽然从原理上也依赖于极限定理，但在类别决策时，只与极少量的相邻样本有关。由于 KNN 方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于&lt;strong&gt;类域的交叉或重叠较多&lt;/strong&gt;的待分样本集来说，KNN 方法较其他方法更为适合。KNN 算法不仅可以用于&lt;strong&gt;分类&lt;/strong&gt;，还可以用于&lt;strong&gt;回归&lt;/strong&gt;。通过找出一个样本的 k 个最近邻居， 将这些邻居的属性的平均值赋给该样本，就可以得到该样本的属性。更有用的方法是将不同距离的邻居对该样本产生的影响给予不同的&lt;strong&gt;权值(weight)&lt;/strong&gt;，如权值与距离成正比。&lt;/p&gt;

&lt;h3 id=&quot;11-knn-for-regression-prediction&quot;&gt;1.1 KNN for Regression (Prediction)&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/knn/knn_9.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;Assume a value for the number of nearest neighbors K and a prediction point x0.&lt;/li&gt;
    &lt;li&gt;KNN identifies the training observations No closest to the prediction point x0.&lt;/li&gt;
    &lt;li&gt;KNN estimates f (x0) using the average of all the responses in N0&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/knn/knn_10.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Q: Any better non-parametric model, do we need to adjust the weights? 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;12-度量问题&quot;&gt;1.2 度量问题&lt;/h3&gt;

&lt;h4 id=&quot;121-distance-measure&quot;&gt;1.2.1 Distance Measure&lt;/h4&gt;

&lt;p&gt;关于距离的度量，两点 A，B 之间的距离d，应该有如下性质：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;1. d(A,B) = d(B,A)		symmery(对称性)
2. d(A,A) = 0		    constancy of self-similarity（自相似性的恒定性）
3. d(A,B) = 0   iff	  A = B		positivity separation（分离性）
4. d(A,B) &amp;lt;= d(A,C) + d(B,C)	trangle inquality
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;以下简单介绍常见的几种距离：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Euclidean Distance（欧几里得距离）
&lt;img src=&quot;/assets/knn/knn_2.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Manhattan Distance（曼哈顿距离）
&lt;img src=&quot;/assets/knn/knn_3.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Minkowski Distance（闵可夫斯基距离)
&lt;img src=&quot;/assets/knn/knn_4.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Hamming Distance（汉明距离）
&lt;img src=&quot;/assets/knn/knn_6.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Cosine Distance
&lt;img src=&quot;/assets/knn/knn_5.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;122-edited-measure&quot;&gt;1.2.2 Edited Measure&lt;/h4&gt;

&lt;blockquote&gt;
  &lt;p&gt;To measure the similarity between two objects, transform one into the other, and measure how much effort it took. The measure of effort becomes the distance measure.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;给定固定特征，看样本在特征上的表现，关注是否相同。
&lt;img src=&quot;/assets/knn/knn_11.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;二-knn-的不足&quot;&gt;二. KNN 的不足&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;该算法在分类时有个主要的不足是，当&lt;strong&gt;样本不平衡&lt;/strong&gt;时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的 K 个邻居中大容量类的样本占多数。因此可以采用权值的方法(和该样本距离小的邻居权值大)来改进。&lt;/li&gt;
  &lt;li&gt;该方法的另一个不足之处是&lt;strong&gt;计算量较大&lt;/strong&gt;，因为对每一个待分类的文本都要计算它到全体已知样本的距离，才能求得它的 K 个最近邻点。目前常用的解决方法是事先对已知样本点进行剪辑，事先去除对分类作用不大的样本。该算法比较适用于样本容量比较大的类域的自动分类，而那 些样本容量较小的类域采用这种算法比较容易产生误分。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;三-knn-中遇到的问题&quot;&gt;三. KNN 中遇到的问题&lt;/h2&gt;

&lt;h3 id=&quot;31-two-questions&quot;&gt;3.1 Two Questions&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;K has to an odd number?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;KNN for &lt;strong&gt;classification&lt;/strong&gt;:  &lt;br /&gt;
如果 K 为&lt;strong&gt;偶数&lt;/strong&gt;，出现的两类别相同，可以根据距离计算离得近的类别，从而进行分类。  &lt;br /&gt;
如果 k 为 &lt;strong&gt;∞&lt;/strong&gt; 时，只需要看两类的个数。  &lt;br /&gt;
如果 k 为 &lt;strong&gt;1&lt;/strong&gt; 时，过拟合。&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;ol&gt;
    &lt;li&gt;KNN for &lt;strong&gt;regression(prediction)&lt;/strong&gt;:  &lt;br /&gt;
求 x 对应点处的 y 值，只需将 x 附近取 K 个点，求 K 个点的 y 的&lt;strong&gt;平均值&lt;/strong&gt;即可。另外取K个均值时，由于每个点的贡献不同，可取相应的&lt;strong&gt;权重&lt;/strong&gt;。&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;What if K becomes very large?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;img src=&quot;/assets/knn/knn_7.png&quot; alt=&quot;1&quot; /&gt;
In two dimensions, the nearest-neighbor algorithm leads to a partitioning of the input space into Voronoi cells, each label led by the category of the training point it contains. In three dimensions, the cells are three-dimensional, and the decision boundary resembles the surface of a crystal.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;img src=&quot;/assets/knn/knn_8.png&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;KNN 这部分的代码在另一个仓库中：
&amp;lt;https://github.com/provenclei/tensorflow_learning_path&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>被水淹死的鱼</name></author><category term="机器学习" /><summary type="html">K-Nearest Neighber algorithm</summary></entry><entry><title type="html">Markdown语法</title><link href="http://localhost:4000/2018/10/06/Markdown%E8%AF%AD%E6%B3%95.html" rel="alternate" type="text/html" title="Markdown语法" /><published>2018-10-06T00:00:00+08:00</published><updated>2018-10-06T00:00:00+08:00</updated><id>http://localhost:4000/2018/10/06/Markdown%E8%AF%AD%E6%B3%95</id><content type="html" xml:base="http://localhost:4000/2018/10/06/Markdown%E8%AF%AD%E6%B3%95.html">&lt;h1 id=&quot;markdown-语法和-mweb-写作使用说明&quot;&gt;Markdown 语法和 MWeb 写作使用说明&lt;/h1&gt;

&lt;h2 id=&quot;markdown-的设计哲学&quot;&gt;Markdown 的设计哲学&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;Markdown 的目標是實現「易讀易寫」。
不過最需要強調的便是它的可讀性。一份使用 Markdown 格式撰寫的文件應該可以直接以純文字發佈，並且看起來不會像是由許多標籤或是格式指令所構成。
Markdown 的語法有個主要的目的：用來作為一種網路內容的&lt;em&gt;寫作&lt;/em&gt;用語言。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!-- more --&gt;

&lt;h2 id=&quot;本文约定&quot;&gt;本文约定&lt;/h2&gt;

&lt;p&gt;如果有写 &lt;code class=&quot;highlighter-rouge&quot;&gt;效果如下：&lt;/code&gt;， 在 MWeb 编辑状态下只有用 &lt;code class=&quot;highlighter-rouge&quot;&gt;CMD + R&lt;/code&gt; 预览才可以看效果。&lt;/p&gt;

&lt;h2 id=&quot;标题&quot;&gt;标题&lt;/h2&gt;

&lt;p&gt;Markdown 语法：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# 第一级标题 `&amp;lt;h1&amp;gt;` 
## 第二级标题 `&amp;lt;h2&amp;gt;` 
###### 第六级标题 `&amp;lt;h6&amp;gt;` 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;效果如下：&lt;/p&gt;

&lt;h1 id=&quot;第一级标题-h1&quot;&gt;第一级标题 &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;h1&amp;gt;&lt;/code&gt;&lt;/h1&gt;
&lt;h2 id=&quot;第二级标题-h2&quot;&gt;第二级标题 &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;h2&amp;gt;&lt;/code&gt;&lt;/h2&gt;
&lt;h6 id=&quot;第六级标题-h6&quot;&gt;第六级标题 &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;h6&amp;gt;&lt;/code&gt;&lt;/h6&gt;

&lt;h2 id=&quot;强调&quot;&gt;强调&lt;/h2&gt;

&lt;p&gt;Markdown 语法：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;*这些文字会生成`&amp;lt;em&amp;gt;`*
_这些文字会生成`&amp;lt;u&amp;gt;`_

**这些文字会生成`&amp;lt;strong&amp;gt;`**
__这些文字会生成`&amp;lt;strong&amp;gt;`__
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;在 MWeb 中的快捷键为： &lt;code class=&quot;highlighter-rouge&quot;&gt;CMD + U&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;CMD + I&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;CMD + B&lt;/code&gt;
效果如下：&lt;/p&gt;

&lt;p&gt;&lt;em&gt;这些文字会生成&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;em&amp;gt;&lt;/code&gt;&lt;/em&gt;
&lt;em&gt;这些文字会生成&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;u&amp;gt;&lt;/code&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;这些文字会生成&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;strong&amp;gt;&lt;/code&gt;&lt;/strong&gt;
&lt;strong&gt;这些文字会生成&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;strong&amp;gt;&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;换行&quot;&gt;换行&lt;/h2&gt;

&lt;p&gt;四个及以上空格加回车。
如果不想打这么多空格，只要回车就为换行，请勾选：&lt;code class=&quot;highlighter-rouge&quot;&gt;Preferences&lt;/code&gt; - &lt;code class=&quot;highlighter-rouge&quot;&gt;Themes&lt;/code&gt; - &lt;code class=&quot;highlighter-rouge&quot;&gt;Translate newlines to &amp;lt;br&amp;gt; tags&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;列表&quot;&gt;列表&lt;/h2&gt;

&lt;h3 id=&quot;无序列表&quot;&gt;无序列表&lt;/h3&gt;

&lt;p&gt;Markdown 语法：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;* 项目一 无序列表 `* + 空格键`
* 项目二
	* 项目二的子项目一 无序列表 `TAB + * + 空格键`
	* 项目二的子项目二
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;在 MWeb 中的快捷键为： &lt;code class=&quot;highlighter-rouge&quot;&gt;Option + U&lt;/code&gt;
效果如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;项目一 无序列表 &lt;code class=&quot;highlighter-rouge&quot;&gt;* + 空格键&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;项目二
    &lt;ul&gt;
      &lt;li&gt;项目二的子项目一 无序列表 &lt;code class=&quot;highlighter-rouge&quot;&gt;TAB + * + 空格键&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;项目二的子项目二&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;有序列表&quot;&gt;有序列表&lt;/h3&gt;

&lt;p&gt;Markdown 语法：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;1. 项目一 有序列表 `数字 + . + 空格键`
2. 项目二 
3. 项目三
	1. 项目三的子项目一 有序列表 `TAB + 数字 + . + 空格键`
	2. 项目三的子项目二
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;效果如下：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;项目一 有序列表 &lt;code class=&quot;highlighter-rouge&quot;&gt;数字 + . + 空格键&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;项目二&lt;/li&gt;
  &lt;li&gt;项目三
    &lt;ol&gt;
      &lt;li&gt;项目三的子项目一 有序列表 &lt;code class=&quot;highlighter-rouge&quot;&gt;TAB + 数字 + . + 空格键&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;项目三的子项目二&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;任务列表task-lists&quot;&gt;任务列表（Task lists）&lt;/h3&gt;

&lt;p&gt;Markdown 语法：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- [ ] 任务一 未做任务 `- + 空格 + [ ]`
- [x] 任务二 已做任务 `- + 空格 + [x]`
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;效果如下：&lt;/p&gt;

&lt;ul class=&quot;task-list&quot;&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;任务一 未做任务 &lt;code class=&quot;highlighter-rouge&quot;&gt;- + 空格 + [ ]&lt;/code&gt;&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;任务二 已做任务 &lt;code class=&quot;highlighter-rouge&quot;&gt;- + 空格 + [x]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;图片&quot;&gt;图片&lt;/h2&gt;

&lt;p&gt;Markdown 语法：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;![GitHub set up](http://zh.mweb.im/asset/img/set-up-git.gif)
格式: ![Alt Text](url)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Control + Shift + I&lt;/code&gt; 可插入Markdown语法。
如果是 MWeb 的文档库中的文档，还可以用拖放图片、&lt;code class=&quot;highlighter-rouge&quot;&gt;CMD + V&lt;/code&gt; 粘贴、&lt;code class=&quot;highlighter-rouge&quot;&gt;CMD + Option + I&lt;/code&gt; 导入这三种方式来增加图片。
效果如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://zh.mweb.im/asset/img/set-up-git.gif&quot; alt=&quot;GitHub set up&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;链接&quot;&gt;链接&lt;/h2&gt;

&lt;p&gt;Markdown 语法：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;email &amp;lt;example@example.com&amp;gt;
[GitHub](http://github.com)
自动生成连接  &amp;lt;http://www.github.com/&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Control + Shift + L&lt;/code&gt; 可插入Markdown语法。
如果是 MWeb 的文档库中的文档，拖放或&lt;code class=&quot;highlighter-rouge&quot;&gt;CMD + Option + I&lt;/code&gt; 导入非图片时，会生成连接。
效果如下：&lt;/p&gt;

&lt;p&gt;Email 连接： &lt;a href=&quot;mailto:example@example.com&quot;&gt;example@example.com&lt;/a&gt;
&lt;a href=&quot;http://github.com&quot;&gt;连接标题Github网站&lt;/a&gt;
自动生成连接像： &lt;a href=&quot;http://www.github.com/&quot;&gt;http://www.github.com/&lt;/a&gt; 这样&lt;/p&gt;

&lt;h2 id=&quot;区块引用&quot;&gt;区块引用&lt;/h2&gt;

&lt;p&gt;Markdown 语法：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;某某说:
&amp;gt; 第一行引用
&amp;gt; 第二行费用文字
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;CMD + Shift + B&lt;/code&gt; 可插入Markdown语法。
效果如下：&lt;/p&gt;

&lt;p&gt;某某说:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;第一行引用
第二行费用文字&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;行内代码&quot;&gt;行内代码&lt;/h2&gt;

&lt;p&gt;Markdown 语法：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;像这样即可：`&amp;lt;addr&amp;gt;` `code`
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;CMD + K&lt;/code&gt; 可插入Markdown语法。
效果如下：&lt;/p&gt;

&lt;p&gt;像这样即可：&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;addr&amp;gt;&lt;/code&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;code&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;多行或者一段代码&quot;&gt;多行或者一段代码&lt;/h2&gt;

&lt;p&gt;Markdown 语法：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;```js
function fancyAlert(arg) {
  if(arg) {
    $.facebox({div:'#foo'})
  }

}
```
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;CMD + Shift + K&lt;/code&gt; 可插入Markdown语法。
效果如下：&lt;/p&gt;

&lt;div class=&quot;language-js highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;fancyAlert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;arg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;arg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
		&lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;facebox&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;div&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'#foo'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
	
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;顺序图或流程图&quot;&gt;顺序图或流程图&lt;/h2&gt;

&lt;p&gt;Markdown 语法：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;```sequence
张三-&amp;gt;李四: 嘿，小四儿, 写博客了没?
Note right of 李四: 李四愣了一下，说：
李四--&amp;gt;张三: 忙得吐血，哪有时间写。
```

```flow
st=&amp;gt;start: 开始
e=&amp;gt;end: 结束
op=&amp;gt;operation: 我的操作
cond=&amp;gt;condition: 确认？

st-&amp;gt;op-&amp;gt;cond
cond(yes)-&amp;gt;e
cond(no)-&amp;gt;op
```
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;效果如下（ &lt;code class=&quot;highlighter-rouge&quot;&gt;Preferences&lt;/code&gt; - &lt;code class=&quot;highlighter-rouge&quot;&gt;Themes&lt;/code&gt; - &lt;code class=&quot;highlighter-rouge&quot;&gt;Enable sequence &amp;amp; flow chart&lt;/code&gt; 才会看到效果 ）：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-sequence&quot;&gt;张三-&amp;gt;李四: 嘿，小四儿, 写博客了没?
Note right of 李四: 李四愣了一下，说：
李四--&amp;gt;张三: 忙得吐血，哪有时间写。
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-flow&quot;&gt;st=&amp;gt;start: 开始
e=&amp;gt;end: 结束
op=&amp;gt;operation: 我的操作
cond=&amp;gt;condition: 确认？

st-&amp;gt;op-&amp;gt;cond
cond(yes)-&amp;gt;e
cond(no)-&amp;gt;op
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;更多请参考：&lt;a href=&quot;http://bramp.github.io/js-sequence-diagrams/&quot;&gt;http://bramp.github.io/js-sequence-diagrams/&lt;/a&gt;, &lt;a href=&quot;http://adrai.github.io/flowchart.js/&quot;&gt;http://adrai.github.io/flowchart.js/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;表格&quot;&gt;表格&lt;/h2&gt;

&lt;p&gt;Markdown 语法：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;第一格表头 | 第二格表头
--------- | -------------
内容单元格 第一列第一格 | 内容单元格第二列第一格
内容单元格 第一列第二格 多加文字 | 内容单元格第二列第二格
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;效果如下：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;第一格表头&lt;/th&gt;
      &lt;th&gt;第二格表头&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;内容单元格 第一列第一格&lt;/td&gt;
      &lt;td&gt;内容单元格第二列第一格&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;内容单元格 第一列第二格 多加文字&lt;/td&gt;
      &lt;td&gt;内容单元格第二列第二格&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;删除线&quot;&gt;删除线&lt;/h2&gt;

&lt;p&gt;Markdown 语法：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;加删除线像这样用： ~~删除这些~~
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;效果如下：&lt;/p&gt;

&lt;p&gt;加删除线像这样用： &lt;del&gt;删除这些&lt;/del&gt;&lt;/p&gt;

&lt;h2 id=&quot;分隔线&quot;&gt;分隔线&lt;/h2&gt;

&lt;p&gt;以下三种方式都可以生成分隔线：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;***

*****

- - -
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;效果如下：&lt;/p&gt;

&lt;hr /&gt;

&lt;hr /&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;mathjax&quot;&gt;MathJax&lt;/h2&gt;

&lt;p&gt;Markdown 语法：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;块级公式：
$$	x = \dfrac{-b \pm \sqrt{b^2 - 4ac}}{2a} $$

\\[ \frac{1}{\Bigl(\sqrt{\phi \sqrt{5}}-\phi\Bigr) e^{\frac25 \pi}} =
1+\frac{e^{-2\pi}} {1+\frac{e^{-4\pi}} {1+\frac{e^{-6\pi}}
{1+\frac{e^{-8\pi}} {1+\ldots} } } } \\]

行内公式： $\Gamma(n) = (n-1)!\quad\forall n\in\mathbb N$
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;效果如下（&lt;code class=&quot;highlighter-rouge&quot;&gt;Preferences&lt;/code&gt; - &lt;code class=&quot;highlighter-rouge&quot;&gt;Themes&lt;/code&gt; - &lt;code class=&quot;highlighter-rouge&quot;&gt;Enable MathJax&lt;/code&gt; 才会看到效果）：&lt;/p&gt;

&lt;p&gt;块级公式：
&lt;script type=&quot;math/tex&quot;&gt;x = \dfrac{-b \pm \sqrt{b^2 - 4ac}}{2a}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;\[ \frac{1}{\Bigl(\sqrt{\phi \sqrt{5}}-\phi\Bigr) e^{\frac25 \pi}} =
1+\frac{e^{-2\pi}} {1+\frac{e^{-4\pi}} {1+\frac{e^{-6\pi}}
{1+\frac{e^{-8\pi}} {1+\ldots} } } } \]&lt;/p&gt;

&lt;p&gt;行内公式： $\Gamma(n) = (n-1)!\quad\forall n\in\mathbb N$&lt;/p&gt;

&lt;h2 id=&quot;脚注footnote&quot;&gt;脚注（Footnote）&lt;/h2&gt;

&lt;p&gt;Markdown 语法：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;这是一个脚注：[^sample_footnote]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;效果如下：&lt;/p&gt;

&lt;p&gt;这是一个脚注：&lt;sup id=&quot;fnref:sample_footnote&quot;&gt;&lt;a href=&quot;#fn:sample_footnote&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h2 id=&quot;注释和阅读更多&quot;&gt;注释和阅读更多&lt;/h2&gt;

&lt;!-- comment --&gt;
&lt;!-- more --&gt;
&lt;p&gt;Actions-&amp;gt;Insert Read More Comment &lt;em&gt;或者&lt;/em&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;Command + .&lt;/code&gt;
&lt;strong&gt;注&lt;/strong&gt; 阅读更多的功能只用在生成网站或博客时。&lt;/p&gt;

&lt;h2 id=&quot;toc&quot;&gt;TOC&lt;/h2&gt;

&lt;p&gt;Markdown 语法：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[TOC]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;效果如下：&lt;/p&gt;

&lt;p&gt;[TOC]&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:sample_footnote&quot;&gt;
      &lt;p&gt;这里是脚注信息 &lt;a href=&quot;#fnref:sample_footnote&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>被水淹死的鱼</name></author><category term="工具" /><summary type="html">Markdown 语法和 MWeb 写作使用说明</summary></entry><entry><title type="html">Welcome to Jekyll!</title><link href="http://localhost:4000/2018/05/17/welcome-to-jekyll.html" rel="alternate" type="text/html" title="Welcome to Jekyll!" /><published>2018-05-17T14:05:21+08:00</published><updated>2018-05-17T14:05:21+08:00</updated><id>http://localhost:4000/2018/05/17/welcome-to-jekyll</id><content type="html" xml:base="http://localhost:4000/2018/05/17/welcome-to-jekyll.html">&lt;p&gt;You’ll find this post in your &lt;code class=&quot;highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run &lt;code class=&quot;highlighter-rouge&quot;&gt;jekyll serve&lt;/code&gt;, which launches a web server and auto-regenerates your site when a file is updated.&lt;/p&gt;

&lt;p&gt;To add new posts, simply add a file in the &lt;code class=&quot;highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory that follows the convention &lt;code class=&quot;highlighter-rouge&quot;&gt;YYYY-MM-DD-name-of-post.ext&lt;/code&gt; and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.&lt;/p&gt;

&lt;p&gt;Jekyll also offers powerful support for code snippets:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Hi, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'Tom'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#=&amp;gt; prints 'Hi, Tom' to STDOUT.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Check out the &lt;a href=&quot;https://jekyllrb.com/docs/home&quot;&gt;Jekyll docs&lt;/a&gt; for more info on how to get the most out of Jekyll. File all bugs/feature requests at &lt;a href=&quot;https://github.com/jekyll/jekyll&quot;&gt;Jekyll’s GitHub repo&lt;/a&gt;. If you have questions, you can ask them on &lt;a href=&quot;https://talk.jekyllrb.com/&quot;&gt;Jekyll Talk&lt;/a&gt;.&lt;/p&gt;</content><author><name>true</name></author><category term="jekyll" /><summary type="html">You’ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve, which launches a web server and auto-regenerates your site when a file is updated.</summary></entry><entry><title type="html">Hello Jekyll</title><link href="http://localhost:4000/2017/04/18/hello-jekyll.html" rel="alternate" type="text/html" title="Hello Jekyll" /><published>2017-04-18T00:00:00+08:00</published><updated>2017-04-18T00:00:00+08:00</updated><id>http://localhost:4000/2017/04/18/hello-jekyll</id><content type="html" xml:base="http://localhost:4000/2017/04/18/hello-jekyll.html">&lt;blockquote&gt;
  &lt;p&gt;Transform your plain text into static websites and blogs.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;welcome&quot;&gt;Welcome&lt;/h1&gt;

&lt;h2 id=&quot;welcome-1&quot;&gt;Welcome&lt;/h2&gt;

&lt;h3 id=&quot;welcome-2&quot;&gt;Welcome&lt;/h3&gt;

&lt;p&gt;This site aims to be a comprehensive guide to Jekyll. We’ll cover topics such as getting your site up and running, creating and managing your content, customizing the way your site works and looks, deploying to various environments, and give you some advice on participating in the future development of Jekyll itself.&lt;/p&gt;

&lt;h3 id=&quot;so-what-is-jekyll-exactlypermalink&quot;&gt;So what is Jekyll, exactly?Permalink&lt;/h3&gt;

&lt;p&gt;Jekyll is a simple, blog-aware, static site generator. It takes a template directory containing raw text files in various formats, runs it through a converter (like &lt;a href=&quot;https://daringfireball.net/projects/markdown/&quot;&gt;Markdown&lt;/a&gt;) and our &lt;a href=&quot;https://github.com/Shopify/liquid/wiki&quot;&gt;Liquid&lt;/a&gt; renderer, and spits out a complete, ready-to-publish static website suitable for serving with your favorite web server. Jekyll also happens to be the engine behind GitHub Pages, which means you can use Jekyll to host your project’s page, blog, or website from GitHub’s servers for free.&lt;/p&gt;

&lt;h3 id=&quot;helpful-hintspermalink&quot;&gt;Helpful HintsPermalink&lt;/h3&gt;

&lt;p&gt;Throughout this guide there are a number of small-but-handy pieces of information that can make using Jekyll easier, more interesting, and less hazardous. Here’s what to look out for.&lt;/p&gt;

&lt;h3 id=&quot;video-test&quot;&gt;Video Test&lt;/h3&gt;

&lt;iframe type=&quot;text/html&quot; width=&quot;100%&quot; height=&quot;385&quot; src=&quot;http://www.youtube.com/embed/gfmjMWjn-Xg&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;</content><author><name>Jekyll</name></author><category term="jekyll" /><summary type="html">Transform your plain text into static websites and blogs. Welcome Welcome Welcome This site aims to be a comprehensive guide to Jekyll. We’ll cover topics such as getting your site up and running, creating and managing your content, customizing the way your site works and looks, deploying to various environments, and give you some advice on participating in the future development of Jekyll itself. So what is Jekyll, exactly?Permalink Jekyll is a simple, blog-aware, static site generator. It takes a template directory containing raw text files in various formats, runs it through a converter (like Markdown) and our Liquid renderer, and spits out a complete, ready-to-publish static website suitable for serving with your favorite web server. Jekyll also happens to be the engine behind GitHub Pages, which means you can use Jekyll to host your project’s page, blog, or website from GitHub’s servers for free. Helpful HintsPermalink Throughout this guide there are a number of small-but-handy pieces of information that can make using Jekyll easier, more interesting, and less hazardous. Here’s what to look out for. Video Test</summary></entry></feed>