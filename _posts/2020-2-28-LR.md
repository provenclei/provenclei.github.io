---
layout: post
title: '逻辑回归'
date: 2020-2-28
author: 被水淹死的鱼
color: red
cover: 'https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1587293950941&di=9afb04e78f754b0d19e01c707913f985&imgtype=0&src=http%3A%2F%2Fimgtec.eetrend.com%2Fsites%2Fimgtec.eetrend.com%2Ffiles%2F201808%2Fblog%2F17085-35818-shenjingwangluo3.jpg%3F1534815154'
tags: 机器学习 算法
---

# 逻辑回归概述与实战

目录：
* 目录
{:toc}

## 一. 概述
>分类技术是机器学习和数据挖掘应用中的重要组成部分。在数据科学中，大约70%的问题属于分类问题。解决分类问题的算法也有很多种，比如:k-近邻算法，使用距离计算来实现分类;决策树，通过构建直观易懂的树来实现分类;朴素贝叶斯，使用概率论构建分类器。这里我们要讲的是Logistic回归，它是一种很常见的用来解决二元分类问题的方法，它主要是通过寻找最优参数来正确地分类原始数据。

### 1. Logistics Regression

逻辑回归(Logistic Regression,简称LR)，其实是一个很有误导性的概念，虽然它的名字中带有“回归”两个字，但是它最擅长处理的却是分类问题。LR分类器适用于各项广义上的分类任务，例如:评论信息的正负情感分析(二分类)、用户点击率(二分类)、用户违约信息预测(二分类)、垃圾邮件检测(二分类)、疾病预测(二分类)、 用户等级分类(多分类)等场景。

#### 1.1 线性回归
提到逻辑回归我们不得不提一下线性回归，逻辑回归和线性回归同属于广义线性模型，逻辑回归就是用线性回归模型的预测值去拟合真实标签的对数几率(一个事件的几率(odds)是指该事件发生的概率与不发生的概率之比，如果该事件发生的概率是 P，那么该事件的几率是 $\frac{P}{1-P}$ ，对数几率就是 $log\frac{P}{1-P}$)。

逻辑回归和线性回归本质上都是得到一条直线，不同的是，线性回归的直线是尽可能去拟合输入变量X的分布，使得训练集中所有样本点到直线的距离最短;而逻辑回归的直线是尽可能去拟合**决策边界**，使得训练集样本中的样本点尽可能分离开。因此，两者的目的是不同的。

#### 1.2 Sigmoid函数

我们想要的函数应该是，能接受所有的输入然后预测出类别。例如在二分类的情况下，函数能输出0或1。那拥有这 类性质的函数称为海维赛德阶跃函数(Heaviside step function)，又称之为单位阶跃函数(如下图所示):
![1](/assets/lr/lr_1.png) 

单位阶跃函数的问题在于:
在0点位置该函数从0瞬间跳跃到1，这个瞬间跳跃过程很难处理(不好求导)。幸运的是，Sigmoid函数也有类似的性质，且数学上更容易处理。

Sigmoid函数公式:
$$
    f(x) = \frac{1}{1+e^{-x}}
$$

```python
import numpy as np
import math
import matplotlib.pyplot as plt
%matplotlib inline

X = np.linspace(-5,5,200)
y = [1/(1+math.e**(-x)) for x in X]
plt.plot(X,y)
plt.show()

X = np.linspace(-60,60,200)
y = [1/(1+math.e**(-x)) for x in X]
plt.plot(X,y)
plt.show()
```

>当x为0时，Sigmoid函数值为0.5。随着x的增大，对应的函数值将逼近于1;而随着x的减小，函数值逼近于0。所以Sigmoid函数值域为(0,1)，注意这是开区间，它仅无限接近0和1。如果横坐标刻度足够大，Sigmoid函数看起来就很像一个阶跃函数了。

![1](/assets/lr/lr_2.png) 

#### 1.3 逻辑回归
通过将线性模型和Sigmoid函数结合，我们可以得到逻辑回归的公式:
$$
    f(x) = \frac{1}{1+e^{-(wx+b)}}
$$
这样y的取值为(0,1)。对式子进行变换，可得对数几率公式:
$$
    log\frac{y}{1-y} = wx+b
$$
**二项 Logistics 回归:**
$$
    P(y=0 | x) = \frac{1}{1+e^{wx+b}}
$$
$$
    P(y=1 | x) = \frac{e^{wx+b}}{1+e^{wx+b}}
$$

**多项 Logistics 回归**
$$
    P(y=k | x) = \frac{e^{w_k x+b}}{1+\sum_{k=1}^{K-1}{e^{w_k x+b}}}
$$
$$
    P(y=K | x) = \frac{1}{1+\sum_{k=1}^{K-1}{e^{w_k x+b}}}
$$

>注意：
>线性回归和逻辑回归是两类模型，逻辑回归是分类模型，线性回归是回归模型，无可比性。

>面试常见问题：
为什么LR使用对数损失二不适用平方损失?

### 2. LR的损失函数



### 3. LR 正则化

### 4. RL 损失函数求解

## 二. 从疝气病症预测病马死亡率
### s

## 三. SKlearn 实现葡萄牙银行机构营销案


## 四. 总结





